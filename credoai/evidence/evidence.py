"""
Wrappers formatting results of evaluator runs for the Credo AI Platform
"""
import json
import pprint
from abc import ABC, abstractproperty
from datetime import datetime
from typing import Tuple

from credoai.utils import ValidationError
from pandas import DataFrame, Series


class Evidence(ABC):
    """Abstract class defining Evidence"""

    def __init__(self, type: str, additional_labels: dict = None, **metadata):
        self.type = type
        self.additional_labels = additional_labels or {}
        self.metadata = metadata
        self.creation_time: str = datetime.utcnow().isoformat()
        self._label = None
        self._validate()

    def __str__(self):
        return pprint.pformat(self.struct())

    def struct(self):
        """Structure of evidence"""
        structure = {
            "type": self.type,
            "label": self.label,
            "data": self.data,
            "generated_at": self.creation_time,
            "metadata": self.metadata,
        }
        return structure

    @property
    def label(self):
        """
        Adds evidence type specific label
        """
        # additional_labels prioritized
        if self._label is None:
            self._label = {**self.base_label, **self.additional_labels}
        return self._label

    @label.setter
    def label(self, value):
        self._label = value

    @abstractproperty
    def base_label(self):
        pass

    @abstractproperty
    def data(self):
        """
        Adds data reflecting additional structure of child classes
        """
        return {}

    def _validate(self):
        pass


class MetricEvidence(Evidence):
    """
    Evidence for Metric:value result type

    Parameters
    ----------
    type : string
        short identifier for metric.
    value : float
        metric value
    confidence_interval : [float, float]
        [lower, upper] confidence interval
    confidence_level : int
        Level of confidence for the confidence interval (e.g., 95%)
    metadata : dict, optional
        Arbitrary keyword arguments to append to metric as metadata. These will be
        displayed in the governance app
    """

    def __init__(
        self,
        type: str,
        value: float,
        confidence_interval: Tuple[float, float] = None,
        confidence_level: int = None,
        additional_labels=None,
        **metadata
    ):
        self.metric_type = type
        self.value = value
        self.confidence_interval = confidence_interval
        self.confidence_level = confidence_level
        super().__init__("metric", additional_labels, **metadata)

    @property
    def data(self):
        return {
            "value": self.value,
            "confidence_interval": self.confidence_interval,
            "confidence_level": self.confidence_level,
        }

    @property
    def base_label(self):
        label = {"metric_type": self.metric_type}
        return label

    def _validate(self):
        if self.confidence_interval and not self.confidence_level:
            raise ValidationError


class TableEvidence(Evidence):
    """
    Evidence for tabular data

    Parameters
    ----------
    data : str
        a pandas DataFrame to use as evidence
    metadata : dict, optional
        Arbitrary keyword arguments to append to metric as metadata. These will be
        displayed in the governance app
    """

    def __init__(
        self, name: str, table_data: DataFrame, additional_labels=None, **metadata
    ):
        self.name = name
        self._data = table_data
        super().__init__("table", additional_labels, **metadata)

    @property
    def data(self):
        return {
            "columns": self._data.columns.tolist(),
            "value": self._data.values.tolist(),
        }

    @property
    def base_label(self):
        label = {"table_name": self.name}
        return label


class DataProfilerEvidence(Evidence):
    """
    Evidence for the result of pandas profiler"""

    def __init__(self, data: dict, additional_labels: dict = None, **metadata):
        self._data = data
        super().__init__("dataset_profiler", additional_labels, **metadata)

    @property
    def data(self):
        data = self._data["results"]
        if isinstance(data, (Series, DataFrame)):
            data = json.loads(data.to_json())
        return data

    @property
    def base_label(self):
        return {}


class ModelProfilerEvidence(Evidence):
    """
    Evidence for the Model Profiler.

    Splits the data into info, parameters and feature names.

    Labeling contains info on where the entries come from: user vs autogenerated
    """

    def __init__(self, data: DataFrame, additional_labels: dict = None, **metadata):
        super().__init__("model_profiler", additional_labels, **metadata)
        self._data = data["results"]

    @property
    def data(self):
        parameters = self._data.loc["parameters"]
        features_names = self._data.loc["feature_names"]
        remaining_info = self._data[
            ~self._data.index.isin(["parameters", "feature_names", "model_name"])
        ]
        return {
            "info": remaining_info.to_dict(),
            "parameters": parameters,
            "features_names": features_names,
        }

    @property
    def base_label(self):
        return {"model_name": self._data.loc["model_name"]}
