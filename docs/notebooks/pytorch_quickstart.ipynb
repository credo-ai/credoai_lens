{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorch Quickstart\n",
    "\n",
    "Have a pytorch model? This notebook will show you how to get it set up in Lens. If this is your first time using Lens, we suggest starting with our [quickstart notebook](https://credoai-lens.readthedocs.io/en/stable/notebooks/quickstart.html) which takes you through the basics with an sklearn model.\n",
    "\n",
    "**Setup**\n",
    "\n",
    "Lens installation instruction can be found on [readthedocs](https://credoai-lens.readthedocs.io/en/stable/pages/setup.html)\n",
    "\n",
    "**Find the code**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!--Documentation related cell please ignore.-->\n",
    "Click <a class=\"reference internal\" download=\"\" href=\"../notebooks/pytorch_quickstart.ipynb\">here</a> to download this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your pytorch model and dataloaders ready.\n",
    " In this tutorial we will emulate the modeling phase by running a quick script. This script following the [MNIST quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) from the pytorch documentation. You can see the full script [here](https://raw.githubusercontent.com/credo-ai/credoai_lens/main/credoai/demo_assets/_fetch_pytorch_mnist_model.py).\n",
    "\n",
    "\n",
    "Because one of the primary use cases for Lens is to easily add Fairness and bias assessment to your pipeline, we are going to make up a sensitive features array\n",
    "for the purpose of demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and X_test, y_test, etc. are defined by this script\n",
    "from credoai.demo_assets import fetch_mnist_model\n",
    "\n",
    "# model and data are defined by this script\n",
    "model, train_dataloader, test_dataloader = fetch_mnist_model()\n",
    "\n",
    "# making up some sensitive features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sensitive_categories = ['black', 'white']\n",
    "sensitive_features = pd.Series(np.random.choice(sensitive_categories, size=len(test_dataloader.dataset)),\n",
    "                                name='color')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lens\n",
    "\n",
    "Below is a basic example where our goal is to evaluate the above model.\n",
    "\n",
    "Briefly, the code is doing four things:\n",
    "\n",
    "* Wrapping ML artifacts (like models and data) in Lens objects\n",
    "* Initializing an instance of Lens. Lens is the main object that performs evaluations. Under the hood, it creates a `pipeline` of evaluations that are run.\n",
    "* Add evaluators to Lens.\n",
    "* Run Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lens and necessary artifacts\n",
    "from credoai.lens import Lens\n",
    "from credoai.artifacts import ClassificationModel, PytorchData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lens, the classes that evaluate models and/or datasets are called `evaluators`. In this example we are interested in evaluating the model's fairness. For this we can use the `ModelFairness` evaluator. We'll\n",
    "also evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.evaluators import ModelFairness, Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a basic example where our goal is to evaluate the above model. We'll break down this code [below](#Breaking-Down-The-Steps).\n",
    "\n",
    "Briefly, the code is doing four things:\n",
    "\n",
    "* Wrapping ML artifacts (like models and data) in Lens objects\n",
    "* Initializing an instance of Lens. Lens is the main object that performs evaluations. Under the hood, it creates a `pipeline` of evaluations that are run.\n",
    "* Add evaluators to Lens.\n",
    "* Run Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_data = PytorchData(\n",
    "    name = 'mnist-data',\n",
    "    dataloader=test_dataloader,\n",
    "    sensitive_features=sensitive_features\n",
    ")\n",
    "\n",
    "credo_model = ClassificationModel(\n",
    "    name='mnist-classifier',\n",
    "    model_like=model,\n",
    "    tags=None\n",
    ")\n",
    "\n",
    "# Initialization of the Lens object\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data)\n",
    "\n",
    "# initialize the evaluator and add it to Lens\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "lens.add(ModelFairness(metrics=metrics))\n",
    "lens.add(Performance(metrics=metrics))\n",
    "\n",
    "# run Lens\n",
    "lens.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
