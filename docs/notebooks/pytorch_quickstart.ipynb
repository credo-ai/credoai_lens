{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorch Quickstart\n",
    "\n",
    "Have a pytorch model? This notebook will show you how to get it set up in Lens. If this is your first time using Lens, we suggest starting with our [quickstart notebook](https://credoai-lens.readthedocs.io/en/stable/notebooks/quickstart.html) which takes you through the basics with an sklearn model.\n",
    "\n",
    "**Setup**\n",
    "\n",
    "Lens installation instruction can be found on [readthedocs](https://credoai-lens.readthedocs.io/en/stable/pages/setup.html)\n",
    "\n",
    "**Find the code**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!--Documentation related cell please ignore.-->\n",
    "Click <a class=\"reference internal\" download=\"\" href=\"../notebooks/pytorch_quickstart.ipynb\">here</a> to download this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your pytorch model and dataloaders ready.\n",
    " In this tutorial we will emulate the modeling phase by running a quick script. This script following the [MNIST quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) from the pytorch documentation. You can see the full script [here](https://raw.githubusercontent.com/credo-ai/credoai_lens/main/credoai/demo_assets/_fetch_pytorch_mnist_model.py).\n",
    "\n",
    "\n",
    "Because one of the primary use cases for Lens is to easily add Fairness and bias assessment to your pipeline, we are going to make up a sensitive features array\n",
    "for the purpose of demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Get MNIST data\n",
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "Define the model\n",
      "Train the model\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.311020  [   64/60000]\n",
      "loss: 2.299083  [ 6464/60000]\n",
      "loss: 2.270861  [12864/60000]\n",
      "loss: 2.265982  [19264/60000]\n",
      "loss: 2.252087  [25664/60000]\n",
      "loss: 2.234039  [32064/60000]\n",
      "loss: 2.237553  [38464/60000]\n",
      "loss: 2.212780  [44864/60000]\n",
      "loss: 2.209331  [51264/60000]\n",
      "loss: 2.178882  [57664/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.177557  [   64/60000]\n",
      "loss: 2.167854  [ 6464/60000]\n",
      "loss: 2.104097  [12864/60000]\n",
      "loss: 2.129525  [19264/60000]\n",
      "loss: 2.082517  [25664/60000]\n",
      "loss: 2.030308  [32064/60000]\n",
      "loss: 2.066048  [38464/60000]\n",
      "loss: 1.988355  [44864/60000]\n",
      "loss: 1.995682  [51264/60000]\n",
      "loss: 1.940706  [57664/60000]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.942122  [   64/60000]\n",
      "loss: 1.919970  [ 6464/60000]\n",
      "loss: 1.792076  [12864/60000]\n",
      "loss: 1.855927  [19264/60000]\n",
      "loss: 1.739932  [25664/60000]\n",
      "loss: 1.688483  [32064/60000]\n",
      "loss: 1.729258  [38464/60000]\n",
      "loss: 1.615170  [44864/60000]\n",
      "loss: 1.640448  [51264/60000]\n",
      "loss: 1.554882  [57664/60000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.605660  [   64/60000]\n",
      "loss: 1.580251  [ 6464/60000]\n",
      "loss: 1.416787  [12864/60000]\n",
      "loss: 1.513136  [19264/60000]\n",
      "loss: 1.371673  [25664/60000]\n",
      "loss: 1.373454  [32064/60000]\n",
      "loss: 1.392390  [38464/60000]\n",
      "loss: 1.310700  [44864/60000]\n",
      "loss: 1.347405  [51264/60000]\n",
      "loss: 1.252274  [57664/60000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.347611  [   64/60000]\n",
      "loss: 1.335627  [ 6464/60000]\n",
      "loss: 1.161156  [12864/60000]\n",
      "loss: 1.282739  [19264/60000]\n",
      "loss: 1.136432  [25664/60000]\n",
      "loss: 1.174464  [32064/60000]\n",
      "loss: 1.188599  [38464/60000]\n",
      "loss: 1.129096  [44864/60000]\n",
      "loss: 1.170482  [51264/60000]\n",
      "loss: 1.085711  [57664/60000]\n"
     ]
    }
   ],
   "source": [
    "# model and X_test, y_test, etc. are defined by this script\n",
    "from credoai.demo_assets import fetch_mnist_model\n",
    "\n",
    "# model and data are defined by this script\n",
    "model, train_dataloader, test_dataloader = fetch_mnist_model()\n",
    "\n",
    "# making up some sensitive features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sensitive_categories = ['black', 'white']\n",
    "sensitive_features = pd.Series(np.random.choice(sensitive_categories, size=len(test_dataloader.dataset)),\n",
    "                                name='color')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lens\n",
    "\n",
    "Below is a basic example where our goal is to evaluate the above model.\n",
    "\n",
    "Briefly, the code is doing four things:\n",
    "\n",
    "* Wrapping ML artifacts (like models and data) in Lens objects\n",
    "* Initializing an instance of Lens. Lens is the main object that performs evaluations. Under the hood, it creates a `pipeline` of evaluations that are run.\n",
    "* Add evaluators to Lens.\n",
    "* Run Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - You are using deepchecks version 0.11.1, however a newer version is available.Deepchecks is frequently updated with major improvements. You should consider upgrading via the \"python -m pip install --upgrade deepchecks\" command.\n"
     ]
    }
   ],
   "source": [
    "# Import Lens and necessary artifacts\n",
    "from credoai.lens import Lens\n",
    "from credoai.artifacts import ClassificationModel, PytorchData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lens, the classes that evaluate models and/or datasets are called `evaluators`. In this example we are interested in evaluating the model's fairness. For this we can use the `ModelFairness` evaluator. We'll\n",
    "also evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.evaluators import ModelFairness, Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a basic example where our goal is to evaluate the above model. We'll break down this code [below](#Breaking-Down-The-Steps).\n",
    "\n",
    "Briefly, the code is doing four things:\n",
    "\n",
    "* Wrapping ML artifacts (like models and data) in Lens objects\n",
    "* Initializing an instance of Lens. Lens is the main object that performs evaluations. Under the hood, it creates a `pipeline` of evaluations that are run.\n",
    "* Add evaluators to Lens.\n",
    "* Run Lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__class__.__bases__[0].__module__.startswith('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-31 14:43:01,052 - lens - INFO - Evaluator ModelFairness added to pipeline. Sensitive feature: SEX\n",
      "2022-10-31 14:43:01,243 - lens - INFO - fairness metric, equal_opportunity, unused by PerformanceModule\n",
      "2022-10-31 14:43:01,250 - lens - INFO - Evaluator Performance added to pipeline. \n",
      "2022-10-31 14:43:01,250 - lens - INFO - Running evaluation for step: PipelineStep(evaluator=<credoai.evaluators.fairness.ModelFairness object at 0x2a5a20fd0>, metadata={'evaluator': 'ModelFairness', 'sensitive_feature': 'SEX', 'dataset_type': 'assessment_data'})\n",
      "2022-10-31 14:43:01,259 - lens - INFO - Running evaluation for step: PipelineStep(evaluator=<credoai.evaluators.performance.Performance object at 0x2a5a3dac0>, metadata={'evaluator': 'Performance'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.lens.Lens at 0x2a5a3da30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "credo_data = PytorchData(\n",
    "    name = 'mnist-data',\n",
    "    dataloader=test_dataloader,\n",
    "    sensitive_features=sensitive_features\n",
    ")\n",
    "\n",
    "credo_model = ClassificationModel(\n",
    "    name='mnist-classifier',\n",
    "    model_like=model,\n",
    "    tags=None\n",
    ")\n",
    "\n",
    "# Initialization of the Lens object\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data)\n",
    "\n",
    "# initialize the evaluator and add it to Lens\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "lens.add(ModelFairness(metrics=metrics))\n",
    "lens.add(Performance(metrics=metrics))\n",
    "\n",
    "# run Lens\n",
    "lens.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
