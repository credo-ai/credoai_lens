{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b77dd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connecting with the Credo AI Governance App\n",
    "\n",
    "Connecting with the Credo AI Governance App is straightforward using Lens. This notebook is a comprehensive overview of how you you can interact with the Governance App.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/governance_integration.ipynb).\n",
    "\n",
    "**Config File**\n",
    "\n",
    "First, ensure you have a config file somewhere on your computer named \".credoconfig\". The default location where Lens will look for it in your home directory (`~/.credoconfig`). The structure of the config should look like this\n",
    "\n",
    "```\n",
    "TENANT=<tenant name> # Example: credoai\n",
    "CREDO_URL=<your credo url> \n",
    "API_KEY=<your api key> # Example: JSMmd26...\n",
    "```\n",
    "\n",
    "This config gives Lens API access to the Governance App. A config file can be downloaded from the web app. MySettings -> Token\n",
    "\n",
    "**Governance App Setup**\n",
    "Connecting Lens to the Governance App requires that you have already defined a Use-Case and registered a model to that use-case. That model serves as the endpoint for Lens.\n",
    "\n",
    "**Data + Model Preparation (before Lens)**\n",
    "\n",
    "Some quick setup. This script reflects all of your datascience work before assessment and integration with Credo AI.\n",
    "\n",
    "Here we have a gradient boosted classifier trained on the UCI Credit Card Default Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddde09b9-b132-4e8f-a411-395876b6389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and df are defined by this script\n",
    "%run training_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583063fe-657c-4478-b71a-3cd9fd03687c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec8ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Lens imports\n",
    "import credoai.lens as cl\n",
    "# set default format for image displays. Change to 'png' if 'svg' is failing\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a77abb-30a0-42a6-ab5b-8d42358d864a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Integration Quickstart\n",
    "\n",
    "Building off of the [quickstart notebook](https://credoai-lens.readthedocs.io/en/latest/notebooks/quickstart.html), let's look at how you can export to a Use Case on the Governance App. We only have to add two new lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680633e8-f3cc-4255-a7bc-d00134919e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_keys=['SEX'],\n",
    "                          label_key='target'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e45d4-5cf1-48be-855c-b632ab69b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below will fail unless these IDs are defined!\n",
    "use_case_id = 'x'  # your use case id can be found in the Governance App\n",
    "model_id = 'x'  # your model id can be found in the Governance App\n",
    "\n",
    "# when interacting with the Governace Platform, the spec_destination looks like this.\n",
    "spec_destination = f'use_cases/{use_case_id}/models/{model_id}/assessment_spec' \n",
    "\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               governance=spec_destination # <- NEW LINE!\n",
    "               )\n",
    "\n",
    "lens.run_assessments().export() # <- Added export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f63c1-ad1a-4f88-be94-4fb89da4f3e6",
   "metadata": {},
   "source": [
    "Those two lines did a number of things under the hood.\n",
    "\n",
    "* First a [CredoGovernance Artifact](#Credo-Governance-Artifact) was created with the credo-url provided.\n",
    "* Next the datasets were registered:\n",
    "    * The dataset was registered to the Governanc App\n",
    "    * The dataset was registered as validation data for the associated model and use-case.\n",
    "* Next an assessment plan was downloaded and updated:\n",
    "    * An assessment plan was downloaded for the model (if it exists).\n",
    "    * The assessment plan was updated with the assessment_plan specified in code here.\n",
    "* Finally, when `export` was called the assessment was exported to the model under the use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce821181",
   "metadata": {
    "tags": []
   },
   "source": [
    "## In-Depth Overview\n",
    "\n",
    "### Credo Governance Artifact\n",
    "\n",
    "Lens connects to the Governance App via a `CredoGovernance` object. All you need to do is provide a `spec-destination`. The `spec-destination` can be one of two things:\n",
    "* A url for the spec in the AI Governance app (which can be found in the Governance App, or in an email requesting a model assessment))\n",
    "* A file path to the spec after it was downloaded from the AI Governance App (an example assessment_plan.json is included in the repo for this package and will be referenced below).\n",
    "\n",
    "**The CredoGovernance object has a few functionalities.**\n",
    "\n",
    "* Retrieving an alignment spec from the Credo AI Governance App. The alignment spec is either downloaded straight from the Governance App based on a credo_url, or supplied as a json file for air gap deployment. \n",
    "    * This alignment spec includes an assessment plan, which can be supplemented (or overwritten) with an assessment plan you define in code and pass to Lens.\n",
    "* Registering artifacts (e.g., datasets) to the Governance App.\n",
    "* Encoding the IDs of governance objects so Lens can easily export metrics and reports.\n",
    "\n",
    "### Retrieving an alignment spec\n",
    "\n",
    "Here we use a local assessment spec in the Credo AI Lens repository. In general, you'd only do this in an air gapped environment. More often, you'd provide an API url that connects Lens with the Governance Platform. That API Url has the following form:\n",
    "\n",
    "```use_cases/{use_case_id}/models/{model_id}/assessment_spec```\n",
    "\n",
    "\n",
    "Regardless, the assessment spec is a primary way in which the Governance App communicates with Lens and defines an assessment strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd91c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict, {'Performance': {'metrics': ['precision_score']}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we reference a local assessment plan, rather than providing a url!\n",
    "# Normally you would only do this in an air-gap environment\n",
    "credo_gov = cl.CredoGovernance(spec_destination = 'assessment_spec.json')\n",
    "\n",
    "# one aspect of the assessment spec is an assessment plan...\n",
    "credo_gov.get_assessment_plan()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee2d06-0863-4274-9618-dcb34d9ca692",
   "metadata": {},
   "source": [
    "### Registering artifacts using CredoGovernance\n",
    "\n",
    "Models and datasets can be registered programatically, with no assessment spec needed. Only a use-case ID must be provided.\n",
    "\n",
    "Assessment templates (assessment plan blueprints saved in the Governance Platform) can be programatically applied to a model once it has been registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48469ea-cfa4-49bd-8411-487e224e4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_gov = cl.CredoGovernance()\n",
    "# the use case must be defined before registering anything!\n",
    "credo_gov.use_case_id = \"Use-Case-Id\"\n",
    "\n",
    "# registering a model and dataset\n",
    "credo_gov.register(model_name = \"My Model\",\n",
    "                   dataset_name = \"My Dataset\")\n",
    "\n",
    "# applying an assessment template to the registered model\n",
    "# Note! This assessment template must exist in the Governance Platform\n",
    "credo_gov.register(assessment_template = \"My Template\")\n",
    "\n",
    "# Registering a model, followed by applying an assessment template to it\n",
    "credo_gov.register(model_name = \"My Model\",\n",
    "                   assessment_template = \"My Template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb999b9",
   "metadata": {},
   "source": [
    "### Interacting with Credo AI's Governance App from Lens\n",
    "\n",
    "In addition to the model and data, we can pass the CredoGovernance object. This is how Lens knows which Use Case, model and/or data to interacts with. If you are running assessments on multiple models, you'll create a separate `CredoGovernance` object (and separate Lens instances) for each.\n",
    "\n",
    "You can also pass the assessment spec location and nothing else. In this case a CredoGovernance object will be created using that destination.\n",
    "\n",
    "**Note**\n",
    "\n",
    "This use assumes that a model is _already_ registered to the governance app with an assessment plan. You can easily do this in the Governance App, or by registering the model first programatically, as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abba24d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running assessment: Performance\n",
      "INFO:absl:Running assessment: Fairness\n"
     ]
    }
   ],
   "source": [
    "from credoai.assessment import FairnessAssessment, PerformanceAssessment\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               governance=credo_gov,\n",
    "               assessments = [PerformanceAssessment, FairnessAssessment])\n",
    "\n",
    "# After running, you just have to export and the metrics \n",
    "# will be sent to the Governance App\n",
    "results = lens.run_assessments().get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3235b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Air Gap Environment\n",
    "\n",
    "If you cannot interact with Credo AI's Governance App via the internet, this section is for you!\n",
    "\n",
    "Instead of Lens automating the communication between your assessment environment and governance, you will instead have to download an assessment_spec, provided it to Lens, and then upload the results assessments. The demo has actually already been using a local example spec included in the github repo.\n",
    "\n",
    "### Getting the assessment spec\n",
    "The asset that you have to get _from_ the governance app is the assessment spec. This can be found under your use case's \"Risk Assessment\" tab inside \"Align\". Once you download the assessments spec, you can read it with a CredoGovernance object by changing one argument.\n",
    "\n",
    "**Important Note On Data Registration in an air-gapped environment** \n",
    "\n",
    "When interacting with Credo AI's API you _only_ have to register a Use Case and Model within the app - nothing neds to be done relating to data. Lens will handle registering your training/assessment data to the app.\n",
    "\n",
    "However, when in an air-gapped environment you WILL need to register the training and validation data in the app first. Otherwise Lens will fail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c82f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance(spec_destination='{path to assessment spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c1749",
   "metadata": {},
   "source": [
    "Following that, you can pass the governance object to Lens as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d98d",
   "metadata": {},
   "source": [
    "### Exporting Results\n",
    "\n",
    "Lens can export assessment results to file as a json object. This is generally useful if you want to store your results locally, but particularly useful if you are in an air gapped environment where you cannot access Credo AI's Governance App directly from your development environment.\n",
    "\n",
    "Doing this only requires a one line change in our current flow. We change the `export` argument from `credoai` to a local path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.run_assessments().export('{path where assessments should be exported}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3f5c2",
   "metadata": {},
   "source": [
    "Running the above will create a folder that you specified, if it doesn't exist. It will then create a json file with the assessment results, which can be uploaded to Credo AI's Governance App. This json file can be uploaded by going to the assessments of your **Use-Case** and pressing the purple \"upload assessment\" button."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
