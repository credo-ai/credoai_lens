{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b77dd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connecting with the Credo AI Governance App\n",
    "\n",
    "Connecting with the Credo AI Governance App is straightforward using Lens. This notebook is a comprehensive overview of how you you can interact with the Governance App.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/governance_integration.ipynb).\n",
    "\n",
    "**Config File**\n",
    "\n",
    "First, ensure you have a config file somewhere on your computer named \".credoconfig\". The default location where Lens will look for it in your home directory (`~/.credoconfig`). The structure of the config should look like this\n",
    "\n",
    "```\n",
    "TENANT=<tenant name> # Example: credoai\n",
    "API_KEY=<your api key> # Example: JSMmd26...\n",
    "```\n",
    "\n",
    "This config gives Lens API access to the Governance App.\n",
    "\n",
    "**Data + Model Preparation (before Lens)**\n",
    "\n",
    "Some quick setup. This script reflects all of your datascience work before assessment and integration with Credo AI.\n",
    "\n",
    "Here we have a gradient boosted classifier trained on the UCI Credit Card Default Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddde09b9-b132-4e8f-a411-395876b6389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# model and df are defined by this script\n",
    "%run training_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583063fe-657c-4478-b71a-3cd9fd03687c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec8ade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/conda_dev/lib/python3.9/site-packages/numba/cpython/hashing.py:484: UserWarning: FNV hashing is not implemented in Numba. See PEP 456 https://www.python.org/dev/peps/pep-0456/ for rationale over not using FNV. Numba will continue to work, but hashes for built in types will be computed using siphash24. This will permit e.g. dictionaries to continue to behave as expected, however anything relying on the value of the hash opposed to hash as a derived property is likely to not work as expected.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Base Lens imports\n",
    "import credoai.lens as cl\n",
    "# set default format for image displays. Change to 'png' if 'svg' is failing\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a77abb-30a0-42a6-ab5b-8d42358d864a",
   "metadata": {},
   "source": [
    "## Integration Quickstart\n",
    "\n",
    "Building off of the [quickstart notebook](https://credoai-lens.readthedocs.io/en/latest/notebooks/quickstart.html), let's look at how you can export to a Use Case on the Governance App. We only have to add two new lines.\n",
    "\n",
    "**Note** The below will fail unless you change \"use-case-id\" to the ID of a use-case actually defined in your Governance app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a2c936-e8a6-4232-b33f-a7f2995cbbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Automatically Selected Assessments for Model without data\n",
      "\n",
      "INFO:absl:Automatically Selected Assessments for model: credit_default_classifier and validation dataset: UCI-credit-default\n",
      "--\n",
      "INFO:absl:Selected assessments...\n",
      "--DatasetFairness\n",
      "--DatasetProfiling\n",
      "--Fairness\n",
      "--Performance\n",
      "INFO:absl:Initializing assessments for validation dataset: UCI-credit-default\n",
      "INFO:absl:fairness metric, equal_opportunity, unused by PerformanceModule\n",
      "INFO:absl:Running assessment-DatasetFairness\n",
      "INFO:absl:Running assessment-DatasetProfiling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d685fa8bf1b49c8afca47b3ad2799ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running assessment-Fairness\n",
      "INFO:absl:Running assessment-Performance\n",
      "WARNING:absl:The model (credit_default_classifier) is already registered. Using registered model\n",
      "INFO:absl:Registering model (credit_default_classifier) to Use Case (VhST9gvC3CWVpWtQKp4wL3)\n",
      "WARNING:absl:The dataset (UCI-credit-default) is already registered. Using registered dataset\n",
      "INFO:absl:Registering dataset (UCI-credit-default) as validation data for model (credit_default_classifier) for use case (al use case1)\n",
      "INFO:absl:** Exporting assessment-DatasetFairness\n",
      "INFO:absl:** Exporting assessment-DatasetProfiling\n",
      "INFO:absl:** Exporting assessment-Fairness\n",
      "INFO:absl:** Exporting assessment-Performance\n",
      "WARNING:absl:No report is included. To include a report, run create_reports first\n",
      "INFO:absl:Exporting assessments to Credo AI's Governance App\n"
     ]
    }
   ],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_key='SEX',\n",
    "                          label_key='target'\n",
    "                          )\n",
    "\n",
    "# specify the metrics that will be used by the Fairness and Performance assessment\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "alignment_spec = {'Fairness': {'metrics': metrics},\n",
    "                  'Performance': {'metrics': metrics}}\n",
    "# run lens\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec,\n",
    "               governance='use-case-name' # <- NEW LINE!\n",
    "               )\n",
    "\n",
    "lens.run_assessments().create_report().export() # <- Added export!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f63c1-ad1a-4f88-be94-4fb89da4f3e6",
   "metadata": {},
   "source": [
    "Those two lines did a number of things under the hood.\n",
    "\n",
    "* First a [CredoGovernance Artifact](#Credo-Governance-Artifact) was created with the use_case_name set to the provided use case's name.\n",
    "* Next the model and datasets were registered:\n",
    "    * The model was registered to the Governance App\n",
    "    * The model was registered to the specific use-case associated with the id\n",
    "    * The data was registered to the Governanc App\n",
    "* Finally, when `export` was called the assessment was exported to the model under the use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce821181",
   "metadata": {
    "tags": []
   },
   "source": [
    "## In-Depth Overview\n",
    "\n",
    "### Credo Governance Artifact\n",
    "\n",
    "Lens connects to the Governance App via a `CredoGovernance` object. This uses the names of the Use Case, model and data you want to associate your assessments with. \n",
    "\n",
    "The CredoGovernance object has a few functionalities.\n",
    "\n",
    "* Retrieving an alignment spec from the Use Case for a specific model (created during the aligned process). The alignment spec is either downloaded straight from the Governance App, or supplied as a json file for air gap deployment. This alignment spec can be supplemented (or overwritten) with an alignment spec you define in code and pass to Lens.\n",
    "* Connecting Lens with the Use Case, model and dataset on the governance app to support easy exporting of metrics and reports.\n",
    "\n",
    "\\** **Attention** \\**\n",
    "\n",
    "The metrics and reports will _either_ be exported to the model and data specified in by `CredoGovernance` (e.g., using the `model_name`) OR a new model and data will be registered on your CredoGovernance app automatically, which will then be used for export. \n",
    "\n",
    "With that said, let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd91c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New artifact for governance\n",
    "credo_gov = cl.CredoGovernance(use_case_name=\"your-use-case-name\")\n",
    "\n",
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_key='SEX',\n",
    "                          label_key='target'\n",
    "                         )\n",
    "\n",
    "# specify the metrics that will be used by the Fairness assessment. \n",
    "# This spec will supplement (or overide!) whatever spec is \n",
    "# pulled down from the Governance App\n",
    "alignment_spec = {\n",
    "    'Fairness': {'metrics': ['precision_score', 'recall_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb999b9",
   "metadata": {},
   "source": [
    "### Interacting with Credo AI's Governance App from Lens\n",
    "\n",
    "In addition to the model and data, we can pass the CredoGovernance object. This is how Lens knows which Use Case, model and/or data to interacts with. If you are running assessments on multiple models, you'll create a separate `CredoGovernance` object (and separate Lens instances) for each.\n",
    "\n",
    "You can also pass the `use case name` and nothing else. In this case a CredoGovernance object will be created and the model/dataset names will be used for registration and exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import FairnessAssessment\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               governance=credo_gov,\n",
    "               assessments = [FairnessAssessment],\n",
    "               spec=alignment_spec)\n",
    "\n",
    "# After running, you just have to export and the metrics \n",
    "# will be sent to the Governance App\n",
    "lens.run_assessments().export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15128b",
   "metadata": {},
   "source": [
    "#### Reports\n",
    "\n",
    "Reporting is a critical feature of Lens. Your assessments must be packaged in an easily digestible way to support downstream governance. As such, Lens supports robust reporting functionality.\n",
    "\n",
    "Lens creates a jupyter notebook report that collates all of your assessment results into one place. It then converts that notebook to HTML and sends it to Credo AI's Governance App, along with your metrics when `export` is called. The report will be associated with that use-case, as well as a model (either the model you supplied to CredoGovernance, or the model [registered](#Registering-a-model-while-assessing-it) by Lens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdae661",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.create_report().export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d19f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Registering a model while assessing it\n",
    "\n",
    "**No model on Credo AI? No Problem**\n",
    "\n",
    "You may not have a model or dataset registered yet in the Governance App to receive your assessments.  If you still want to log a model assessment, Lens will take care of registering a new model on Credo AI's Governance App first. \n",
    "\n",
    "You still need to provide a use-case name however. You can do this by creating a `CredoGovernance` object, or just by supplying the use-case name to the \"governance\" argument, as we do below.\n",
    "\n",
    "The below will create a project called `an example model project` and log the model `an example model ` under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import FairnessAssessment\n",
    "credo_model = cl.CredoModel(name='an example model',\n",
    "                            model=model)\n",
    "\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               assessments = [FairnessAssessment],\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec,\n",
    "               governance='use-case-name')\n",
    "\n",
    "# Note! The below won't export anything unless you supply a use-case id\n",
    "# A warning will be raised to inform you of this.\n",
    "lens.run_assessments().create_report().export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba17f8",
   "metadata": {},
   "source": [
    "The governance object is created for you, and can be interrogated or used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = lens.get_governance()\n",
    "gov.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b888c",
   "metadata": {},
   "source": [
    "\\** **Attention** \\**\n",
    "\n",
    "\n",
    "Note that if you run the above a second time, the model doesn't have to be registered again - it already was registered! Instead, Lens will look up the model name \"an example model\" on the governance app and use that. If you would like to run Lens again for the same model, but store the data somewhere else you will have to change the name of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01328b0f",
   "metadata": {},
   "source": [
    "### Registering Models, Data and Use Cases\n",
    "\n",
    "While Lens will take care of registering a model and data for you if no `model_id` or `data_id` is provided, you can also manually register models yourself using a `CredoGovernance` object.\n",
    "\n",
    "This is helpful if you want to register a model or data without assessing it yet (though you can also do this from the App), or want to have more control over the names of your artifacts. \n",
    "* First, you create a `CredoGovernance` object\n",
    "* Second register your project and model\n",
    "* Third, pass to Lens as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a355b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Registering a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance()\n",
    "gov.register(model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a410ce",
   "metadata": {},
   "source": [
    "#### Registering a Dataset\n",
    "\n",
    "**Datasets** can be registered analogously. This data is the validation data used for the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716759a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov.register(dataset_name='my_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc351b-15f0-488b-a82e-cd36f3f3d4cb",
   "metadata": {},
   "source": [
    "If you use the `training_dataset_name` argument and a model has already been associated with governance, the dataset will be registered as that model's training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b1570-5199-4224-ad34-d38806785914",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov.register(training_dataset_name='my_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40fa55",
   "metadata": {},
   "source": [
    "#### Registering a Model to Use Case\n",
    "\n",
    "**Use Cases** can also be used. If CredoGovernance has a Use Case, the model will automatically be associated with that Use Case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance(use_case_name=\"your-use-case-name\")\n",
    "gov.register(model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a72c18-dd9c-4540-9757-e703552a8a0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860a854-8f76-4a4c-b0ef-549ae1d10717",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance(use_case_name=\"your-use-case-name\")\n",
    "gov.register(training_dataset_name='my_data',\n",
    "             dataset_name='my_data',\n",
    "             model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3235b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Air Gap Environment\n",
    "\n",
    "If you cannot interact with Credo AI's Governance App via the internet, this section is for you!\n",
    "\n",
    "Instead of Lens automating the communication between your assessment environment and governance, you will instead have to download assets and upload them. \n",
    "\n",
    "### Getting the assessment spec\n",
    "The asset that you may have to get _from_ the governance app is the assessment spec. This can be found under your use case's \"Risk Assessment\" tab inside \"Align\". Once you download the assessments spec, you can read it with a CredoGovernance object:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c82f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance()\n",
    "# you must explicitly retrieve the assessment spec \n",
    "# in an air gapped environment!\n",
    "gov.retrieve_assessment_spec('{path to assessment spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c1749",
   "metadata": {},
   "source": [
    "Following that, you can pass the governance object to Lens as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d98d",
   "metadata": {},
   "source": [
    "### Exporting Results\n",
    "\n",
    "Lens can export assessment results to file as a json object. This is generally useful if you want to store your results locally, but particularly useful if you are in an air gapped environment where you cannot access Credo AI's Governance App directly from your development environment.\n",
    "\n",
    "Doing this only requires a one line change in our current flow. We change the `export` argument from `credoai` to a local path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.run_assessments().create_report().export('{path where assessments should be exported}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3f5c2",
   "metadata": {},
   "source": [
    "Running the above will create a folder that you specified, if it doesn't exist. It will then create a json file with the assessment results and the report, which can be uploaded to Credo AI's Governance App. This json file can be uploaded by going to the assessments of your **Use-Case** and pressing the purple \"upload assessment\" button."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
