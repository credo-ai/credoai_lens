{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b77dd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Connecting with the Credo AI Governance App\n",
    "\n",
    "Connecting with the Credo AI Governance App is straightforward using Lens. This notebook is a comprehensive overview of how you you can interact with the Governance App.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/governance_integration.ipynb).\n",
    "\n",
    "**Config File**\n",
    "\n",
    "First, ensure you have a config file somewhere on your computer named \".credoconfig\". The default location where Lens will look for it in your home directory (`~/.credoconfig`). The structure of the config should look like this\n",
    "\n",
    "```\n",
    "TENANT=<tenant name> # Example: credoai\n",
    "API_KEY=<your api key> # Example: JSMmd26...\n",
    "```\n",
    "\n",
    "This config gives Lens API access to the Governance App.\n",
    "\n",
    "**Data + Model Preparation (before Lens)**\n",
    "\n",
    "Some quick setup. This script reflects all of your datascience work before assessment and integration with Credo AI.\n",
    "\n",
    "Here we have a gradient boosted classifier trained on the UCI Credit Card Default Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde09b9-b132-4e8f-a411-395876b6389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and df are defined by this script\n",
    "%run training_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583063fe-657c-4478-b71a-3cd9fd03687c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Lens imports\n",
    "import credoai.lens as cl\n",
    "# set default format for image displays. Change to 'png' if 'svg' is failing\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a77abb-30a0-42a6-ab5b-8d42358d864a",
   "metadata": {},
   "source": [
    "## Integration Quickstart\n",
    "\n",
    "Building off of the [quickstart notebook](https://credoai-lens.readthedocs.io/en/latest/notebooks/quickstart.html), let's look at how you can export to a Use Case on the Governance App. We only have to add two new lines.\n",
    "\n",
    "**Note** The below will fail unless you change \"use-case-id\" to the ID of a use-case actually defined in your Governance app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2c936-e8a6-4232-b33f-a7f2995cbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_key='SEX',\n",
    "                          label_key='target'\n",
    "                          )\n",
    "\n",
    "# specify the metrics that will be used by the Fairness and Performance assessment\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "alignment_spec = {'Fairness': {'metrics': metrics},\n",
    "                  'Performance': {'metrics': metrics}}\n",
    "# run lens\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec,\n",
    "               governance='use-case-id' # <- NEW LINE!\n",
    "               )\n",
    "\n",
    "lens.run_assessments().create_report().export() # <- Added export!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f63c1-ad1a-4f88-be94-4fb89da4f3e6",
   "metadata": {},
   "source": [
    "Those two lines did a number of things under the hood.\n",
    "\n",
    "* First a [CredoGovernance Artifact](#Credo-Governance-Artifact) was created with the use_case_id set to the provided use_case_id.\n",
    "* Next the model and datasets were registered:\n",
    "    * The model was registered to the Governance App\n",
    "    * The model was registered to the specific use-case associated with the id\n",
    "    * The data was registered to the Governanc App\n",
    "* Finally, when `export` was called the assessment was exported to the model under the use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce821181",
   "metadata": {
    "tags": []
   },
   "source": [
    "## In-Depth Overview\n",
    "\n",
    "### Credo Governance Artifact\n",
    "\n",
    "Lens connects to the Governance App via a `CredoGovernance` object. This specifies the IDs of the Use Case, model and data you want to associate your assessments with. The IDs are found in the url of your Use Case or model.\n",
    "\n",
    "For instance, the Use Case ID is found here: `...credo.com/use-cases/{use_case_id}`\n",
    "\n",
    "And the model Id is found here: `...credo.com/models/{model_id}`\n",
    "\n",
    "The CredoGovernance object has a few functionalities.\n",
    "\n",
    "* Retrieving an alignment spec from the Use Case for a specific model ID (created during the aligned process). The alignment spec is either downloaded straight from the Governance App, or supplied as a json file for air gap deployment. This alignment spec can be supplemented (or overwritten) with an alignment spec you define in code and pass to Lens.\n",
    "* Registering artifacts (models, datasets) to the Governance App.\n",
    "* Encoding the IDs of governance objects so Lens can easily export metrics and reports.\n",
    "\n",
    "\\** **Attention** \\**\n",
    "\n",
    "The metrics and reports will _either_ be exported to the model and data specified in by `CredoGovernance` (using the `model_id` and `data_id` arguments) OR a new model and data will be registered on your CredoGovernance app automatically, which will then be used for export. You can also set the ids using the model/dataset _name_ by calling `credo_gov.set_governance_info_by_name`.\n",
    "\n",
    "With that said, let's see how this works in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd91c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New artifact for governance\n",
    "# Because a model id is provided, the model will not be\n",
    "# registered and assessments will be exported to the existing\n",
    "# \"your-model_id\"\n",
    "credo_gov = cl.CredoGovernance(use_case_id=\"your-use-case-id\",\n",
    "                               model_id=\"your-model-id\"\n",
    "                               )\n",
    "\n",
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          data=df,\n",
    "                          sensitive_feature_key='SEX',\n",
    "                          label_key='target'\n",
    "                         )\n",
    "\n",
    "# specify the metrics that will be used by the Fairness assessment. \n",
    "# This spec will supplement (or overide!) whatever spec is \n",
    "# pulled down from the Governance App\n",
    "alignment_spec = {\n",
    "    'Fairness': {'metrics': ['precision_score', 'recall_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb999b9",
   "metadata": {},
   "source": [
    "### Interacting with Credo AI's Governance App from Lens\n",
    "\n",
    "In addition to the model and data, we can pass the CredoGovernance object. This is how Lens knows which Use Case, model and/or data to interacts with. If you are running assessments on multiple models, you'll create a separate `CredoGovernance` object (and separate Lens instances) for each.\n",
    "\n",
    "You can also pass the `use-case ID` and nothing else. In this case a CredoGovernance object will be created and the model/dataset names will be used for registration and exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import FairnessAssessment\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               governance=credo_gov,\n",
    "               assessments = [FairnessAssessment],\n",
    "               spec=alignment_spec)\n",
    "\n",
    "# After running, you just have to export and the metrics \n",
    "# will be sent to the Governance App\n",
    "lens.run_assessments().export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15128b",
   "metadata": {},
   "source": [
    "#### Reports\n",
    "\n",
    "Reporting is a critical feature of Lens. Your assessments must be packaged in an easily digestible way to support downstream governance. As such, Lens supports robust reporting functionality.\n",
    "\n",
    "Lens creates a jupyter notebook report that collates all of your assessment results into one place. It then converts that notebook to HTML and sends it to Credo AI's Governance App, along with your metrics when `export` is called. The report will be associated with that use-case, as well as a model (either the model you supplied to CredoGovernance, or the model [registered](#Registering-a-model-while-assessing-it) by Lens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdae661",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.create_report().export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d19f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Registering a model while assessing it\n",
    "\n",
    "**No model on Credo AI? No Problem**\n",
    "\n",
    "You may not have a model or dataset registered yet in the Governance App to receive your assessments.  If you still want to log a model assessment, Lens will take care of registering a new model on Credo AI's Governance App first. \n",
    "\n",
    "You still need to provide a use-case ID however. You can do this by creating a `CredoGovernance` object, or just by supplying the use-case ID to the \"governance\" argument, as we do below.\n",
    "\n",
    "The below will create a project called `an example model project` and log the model `an example model ` under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.assessment import FairnessAssessment\n",
    "credo_model = cl.CredoModel(name='an example model',\n",
    "                            model=model)\n",
    "\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               assessments = [FairnessAssessment],\n",
    "               data=credo_data,\n",
    "               spec=alignment_spec,\n",
    "               governance='use-case-id')\n",
    "\n",
    "# Note! The below won't export anything unless you supply a use-case id\n",
    "# A warning will be raised to inform you of this.\n",
    "lens.run_assessments().create_report().export()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba17f8",
   "metadata": {},
   "source": [
    "The governance object is created for you, and can be interrogated or used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = lens.get_governance()\n",
    "gov.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b888c",
   "metadata": {},
   "source": [
    "\\** **Attention** \\**\n",
    "\n",
    "\n",
    "Note that if you run the above a second time, the model doesn't have to be registered again - it already was registered! Instead, Lens will find the ID for model name \"an example model\" and use that (this is because model names must be unique, and are only associated with one ID). If you would like to run Lens again for the same model, but store the data somewhere else you will have to either change the name of the model or explicitly supply a different model_id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01328b0f",
   "metadata": {},
   "source": [
    "### Registering Models, Data and Use Cases\n",
    "\n",
    "While Lens will take care of registering a model and data for you if no `model_id` or `data_id` is provided, you can also manually register models yourself using a `CredoGovernance` object.\n",
    "\n",
    "This is helpful if you want to register a model or data without assessing it yet (though you can also do this from the App), or want to have more control over the names of your artifacts. \n",
    "* First, you create a `CredoGovernance` object\n",
    "* Second register your project and model\n",
    "* Third, pass to Lens as normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a355b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Registering a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance()\n",
    "gov.register(model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a410ce",
   "metadata": {},
   "source": [
    "#### Registering a Dataset\n",
    "\n",
    "**Datasets** can be registered analogously. This data is the validation data used for the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716759a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov.register(dataset_name='my_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc351b-15f0-488b-a82e-cd36f3f3d4cb",
   "metadata": {},
   "source": [
    "If you use the `training_dataset_name` argument and a model has already been associated with governance, the dataset will be registered as that model's training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b1570-5199-4224-ad34-d38806785914",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov.register(training_dataset_name='my_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40fa55",
   "metadata": {},
   "source": [
    "#### Registering a Model to Use Case\n",
    "\n",
    "**Use Cases** can also be used. If CredoGovernance has a Use Case ID, the model will automatically be associated with that Use Case ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance(use_case_id=\"your-use-case-id\")\n",
    "gov.register(model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a72c18-dd9c-4540-9757-e703552a8a0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860a854-8f76-4a4c-b0ef-549ae1d10717",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance(use_case_id=\"your-use-case-id\")\n",
    "gov.register(training_dataset_name='my_data',\n",
    "             dataset_name='my_data',\n",
    "             model_name='my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3235b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Air Gap Environment\n",
    "\n",
    "If you cannot interact with Credo AI's Governance App via the internet, this section is for you!\n",
    "\n",
    "Instead of Lens automating the communication between your assessment environment and governance, you will instead have to download assets and upload them. \n",
    "\n",
    "### Getting the assessment spec\n",
    "The asset that you may have to get _from_ the governance app is the assessment spec. This can be found under your use case's \"Risk Assessment\" tab inside \"Align\". Once you download the assessments spec, you can read it with a CredoGovernance object:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c82f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = cl.CredoGovernance()\n",
    "# you must explicitly retrieve the assessment spec \n",
    "# in an air gapped environment!\n",
    "gov.retrieve_assessment_spec('{path to assessment spec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c1749",
   "metadata": {},
   "source": [
    "Following that, you can pass the governance object to Lens as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d98d",
   "metadata": {},
   "source": [
    "### Exporting Results\n",
    "\n",
    "Lens can export assessment results to file as a json object. This is generally useful if you want to store your results locally, but particularly useful if you are in an air gapped environment where you cannot access Credo AI's Governance App directly from your development environment.\n",
    "\n",
    "Doing this only requires a one line change in our current flow. We change the `export` argument from `credoai` to a local path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5efbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.run_assessments().create_report().export('{path where assessments should be exported}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d3f5c2",
   "metadata": {},
   "source": [
    "Running the above will create a folder that you specified, if it doesn't exist. It will then create a json file with the assessment results and the report, which can be uploaded to Credo AI's Governance App. This json file can be uploaded by going to the assessments of your **Use-Case** and pressing the purple \"upload assessment\" button."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
