{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d608c2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Quickstart\n",
    "\n",
    "Get started here. We will assess a payment default prediction model for gender fairness using Lens, in 5 minutes. More in-depth information can be found in the [lens FAQ](https://credoai-lens.readthedocs.io/en/latest/notebooks/lens_faq.html#How-can-I-choose-which-assessments-to-run?)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Setup instruction can be found on [readthedocs](https://credoai-lens.readthedocs.io/en/stable/setup.html)\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/quickstart.ipynb).\n",
    "\n",
    "**Data + Model Preparation (before Lens)**\n",
    "\n",
    "Some quick setup. This script reflects all of your datascience work before assessment and integration with Credo AI.\n",
    "\n",
    "Here we have a gradient boosted classifier trained on the UCI Credit Card Default Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f914dde-164c-40e0-8dc3-f0f5e00121ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and df are defined by this script\n",
    "%run training_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557a63e-a7d8-4ade-9f71-71416d1abe2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1261860b-4ec3-42e1-872a-d357e22bd8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Base credo imports\n",
    "import credoai \n",
    "print(credoai.__version__)\n",
    "\n",
    "# Import Lens and necessary artifacts\n",
    "from credoai.lens import Lens\n",
    "from credoai.artifacts import ClassificationModel, TabularData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0797a51",
   "metadata": {},
   "source": [
    "## Lens in 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce779fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below is a basic example where our goal is to evaluate the above model.\n",
    "We will be evaluating model fairness according to three different metrics.\n",
    "\n",
    "We'll break down what the artifacts are [below](#Breaking-Down-The-Steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b4e6a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up model and data artifacts\n",
    "credo_model = ClassificationModel(name=\"credit_default_classifier\", model_like=model)\n",
    "credo_data = TabularData(\n",
    "    name=\"UCI-credit-default\",\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    sensitive_features=sensitive_features_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5f9e9",
   "metadata": {},
   "source": [
    "In lens, the modules that evaluate models and/or datasets are called `evaluators`. Since in this case we are interested in evaluate model fairness, we will be importing the `ModelFairness` evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d780fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.evaluators import ModelFairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc7c02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the Lens object\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de9cc0d",
   "metadata": {},
   "source": [
    "The next step is to add the evaluator to the initialized lens object. Notice that when we add an evaluator to the lens pipeline, it is initialized with all the necessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69a4b841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 11:58:48,071 - lens - INFO - Evaluator Fairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.lens.Lens at 0x28a584c40>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "lens.add(ModelFairness(metrics=metrics), id='MyModelFairness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920b8eb",
   "metadata": {},
   "source": [
    "Let's run the pipeline and visualize the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef47c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 11:59:14,096 - lens - INFO - Running evaluation for step: MyModelFairness\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equal_opportunity</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score</td>\n",
       "      <td>0.012715</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                type     value   subtype\n",
       "0  equal_opportunity  0.015972  fairness\n",
       "1    precision_score  0.012715    parity\n",
       "2       recall_score  0.015972    parity"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens.run()\n",
    "lens.get_results()['MyModelFairness'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1eab0",
   "metadata": {},
   "source": [
    "`lens.get_results()` provides a dictionary where the results of the evaluators are stored as values, and the keys correspond to the ids of the evaluators.  \n",
    "\n",
    "In the previous case we specified the id of the evaluator when we added `ModelFairness` to the pipeline, however `id` is an optional argument for the `add` method. If omitted, a random one will be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca7568",
   "metadata": {},
   "source": [
    "## Adding multiple evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a11b3",
   "metadata": {},
   "source": [
    "If we want to add multiple evaluators to our pipeline. One way of doing it could be repeating the `add` step shown above. Another way is to define the pipeline steps, and pass it to `Lens` at initialization time. Let's explore the latter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a6604c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another evaluator that assess fairness in your dataset\n",
    "from credoai.evaluators import DataFairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f506b7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:15:50,006 - lens - INFO - Evaluator Fairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n",
      "2022-09-26 12:15:50,010 - lens - INFO - Evaluator DataFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    (ModelFairness(metrics), \"MyModelFairness\"),\n",
    "    (DataFairness(), \"MyDataFairness\"),\n",
    "]\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data, pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c46bdd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-26 12:16:23,229 - lens - INFO - Running evaluation for step: MyModelFairness\n",
      "2022-09-26 12:16:23,239 - lens - INFO - Running evaluation for step: MyDataFairness\n"
     ]
    }
   ],
   "source": [
    "lens.run()\n",
    "results = lens.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f5203",
   "metadata": {},
   "source": [
    "Let's check that we have results for both of our evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab0ed632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>value</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equal_opportunity</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>fairness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision_score</td>\n",
       "      <td>0.012715</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall_score</td>\n",
       "      <td>0.015972</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                type     value   subtype\n",
       "0  equal_opportunity  0.015972  fairness\n",
       "1    precision_score  0.012715    parity\n",
       "2       recall_score  0.015972    parity"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['MyModelFairness'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5bb680a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>target</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.183853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sensitive_feature</td>\n",
       "      <td>prediction_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>demographic_parity</td>\n",
       "      <td>difference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.880573</td>\n",
       "      <td>1.0</td>\n",
       "      <td>demographic_parity</td>\n",
       "      <td>ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>proxy_mutual_information</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      value  target                      type           subtype\n",
       "0  0.183853     NaN         sensitive_feature  prediction_score\n",
       "1  0.027886     1.0        demographic_parity        difference\n",
       "2  0.880573     1.0        demographic_parity             ratio\n",
       "3  0.025036     NaN  proxy_mutual_information               max"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['MyDataFairness'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8dd11-bc17-42b4-b73f-a42324f31780",
   "metadata": {},
   "source": [
    "## In-Depth Overview\n",
    "\n",
    "CredoAI Lens is the assessment framework component of the broader CredoAI suite.\n",
    "It is usable as a standalone gateway to assessments or in combination\n",
    "with CredoAI's Governance App. \n",
    "\n",
    "Understanding how your AI systems are operating is the most important step in intervening upon your system. From the technically complicated questions of improving a system to the business-relevant questions of whether to deploy a system - everything is fundamentally founded upon good observability. Lens strives to make assessment comprehensive, easy, and adaptable. The primary outputs from Lens are **assessment results** in the form of various metrics. Lens also can visualize some of these results.\n",
    "\n",
    "### Assessments\n",
    "\n",
    "CredoAI Lens is an entry point to assessments developed by CredoAI, as well as the broader ecosystem of open-source assessments. Custom analytics can also be folded in easily (see the `lens customization` notebook)\n",
    "\n",
    "AI system assessment starts with verifying standard performance metrics to an evolving set of assessments falling under the banner of *Responsible AI*. A non-exhaustive list includes\n",
    "\n",
    "* Fairness\n",
    "* Explainability\n",
    "* Performance\n",
    "* Robustness\n",
    "\n",
    "These different categories of assessment differ substantially based on whether one is \n",
    "evaluating datasets or models, what kind of model (e.g., tabular, NLP, computer vision), and the use-case. As the ecosystem develops, Lens will support assessing a broader range of AI systems. Currently, we are focused on Fairness.\n",
    "\n",
    "### Governance\n",
    "\n",
    "While Lens is a stand-alone assessment framework, its value is increased when combined with the CredoAI Governance App. The app supports multi-stakeholder `Alignment` on how to assess your AI systems (e.g., what does good look like for this system?). It also supports translating assessment results into a Risk perspective that is scalable across your organization and understandable to diverse stakeholders.\n",
    "\n",
    "Check out the [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/latest/notebooks/governance_integration.html) for information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c05dfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Breaking Down The Steps\n",
    "\n",
    "### Preparing artifacts\n",
    "\n",
    "Lens interacts with Credo Artifacts which wrap models and datasets and standardizes them for use by different assessments.\n",
    "Below we create a `CredoModel` object, which automatically infers that the \"model\" object is from scikit-learn. We also create a `CredoData` object which is store X, y and sensitive features. Both of these objects are customizable. See `lens_customization.ipynb` for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4f1fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = cl.CredoModel(name='credit_default_classifier',\n",
    "                            model=model)\n",
    "\n",
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          X=X_test,\n",
    "                          y=y_test,\n",
    "                          sensitive_features=sensitive_features_test\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bd0bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CredoModel\n",
    "\n",
    "CredoModel serves as an adapter between arbitrary models and the assessments in CredoLens. Assessments depend on CredoModel instantiating certain methods. In turn, the methods an instance of CredoModel defines informs Lens which assessment can be automatically run.\n",
    "\n",
    "The way a CredoModel works is by defining a \"config\" dictionary that outlines the models functionality.\n",
    "\n",
    "Above the CredoModel functionality was inferred from the fact that the model (GraidentBoostingClassifier) is a scikit-learn model. But under the hood all that happens was it defined a `config`.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1b180fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predict': <bound method ForestClassifier.predict of RandomForestClassifier()>,\n",
       " 'predict_proba': <function credoai.artifacts.CredoModel._sklearn_style_config.<locals>.predict_proba(X)>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the config was inferred from the model passed to CredoModel\n",
    "credo_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fb09b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### CredoData\n",
    "\n",
    "Just as CredoModel is an adapter between arbitrary models and the Lens assessment framework, CredoData serves as an adapter between tabular datasets and the assessments in CredoLens.\n",
    "\n",
    "When you pass a dataframe to CredoData, CredoData separates it into an \"X\", \"y\", and, if applicable, \"sensitive_features\".\n",
    "\n",
    "You can pass CredoData to Lens as a training dataset or a validation dataset. If the former, it will not be used to assess the model. Instead, dataset assessments will be performed on the dataset (e.g., fairness assessment). The validation dataset will be assessed in the same way, but _also_ used to assess the model, if provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc23c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_1</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>11581.0</td>\n",
       "      <td>12580.0</td>\n",
       "      <td>13716.0</td>\n",
       "      <td>14828.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>116684.0</td>\n",
       "      <td>101581.0</td>\n",
       "      <td>77741.0</td>\n",
       "      <td>77264.0</td>\n",
       "      <td>4486.0</td>\n",
       "      <td>4235.0</td>\n",
       "      <td>3161.0</td>\n",
       "      <td>2647.0</td>\n",
       "      <td>2669.0</td>\n",
       "      <td>2669.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23397</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>68530.0</td>\n",
       "      <td>69753.0</td>\n",
       "      <td>70111.0</td>\n",
       "      <td>70212.0</td>\n",
       "      <td>2431.0</td>\n",
       "      <td>3112.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2438.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2554.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LIMIT_BAL  EDUCATION  MARRIAGE  AGE  PAY_1  PAY_2  PAY_3  PAY_4  PAY_5  \\\n",
       "2308           2          2         2    4      2      2      2      2      2   \n",
       "22404         14          1         2    5      2      2      2      2      2   \n",
       "23397          6          3         1   11      2      2      2      2      2   \n",
       "\n",
       "       PAY_6  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  \\\n",
       "2308       2  ...    11581.0    12580.0    13716.0    14828.0    1500.0   \n",
       "22404      2  ...   116684.0   101581.0    77741.0    77264.0    4486.0   \n",
       "23397      2  ...    68530.0    69753.0    70111.0    70212.0    2431.0   \n",
       "\n",
       "       PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "2308     2000.0    1500.0    1500.0    1500.0    2000.0  \n",
       "22404    4235.0    3161.0    2647.0    2669.0    2669.0  \n",
       "23397    3112.0    3000.0    2438.0    2500.0    2554.0  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credo_data = cl.CredoData(name='UCI-credit-default',\n",
    "                          X=X_test,\n",
    "                          y=y_test,\n",
    "                          sensitive_features=sensitive_features_test\n",
    "                          )\n",
    "credo_data.X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4583249",
   "metadata": {},
   "source": [
    "### Assessments \n",
    "\n",
    "Lens uses the functionality of the above artifacts to automatically determine which assessments can be run. In this case the Dataset Assessment and Fairness Assessment can be run. You can see what assessments are runnable with the following function.\n",
    "\n",
    "Assessments can be chosen, rather than inferred. See the [lens FAQ](https://credoai-lens.readthedocs.io/en/latest/notebooks/lens_faq.html#How-can-I-choose-which-assessments-to-run?) for this functionality, and other information about assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5bf3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Assessment Plan\n",
    "\n",
    "The Assessment Plan describes how the assessments should be run. Think about it is as the *parameterization* of the assessments Lens will run.\n",
    "\n",
    "If you use the Credo AI Governance App, the Assessment Plan is a principle artifact determined during the *Alignment Phase*. It is the output of multi-stakeholder collaboration. Lens will automatically download the Assessment Plan associated with your governance credentials (which uses another artifact: `CredoGovernance`)\n",
    "\n",
    "You can also define the plan in code. Anything defined in the `assessment_plan` parameter will take precedence over the Assessment Plan retrieved from the Governance App.\n",
    "\n",
    "**Setting up the Plan**\n",
    "\n",
    "The Assessment Plan is a set of {assessment_name: parameter} pairs. The assessment name must be the name of one of the assessments, as returned by `get_usable_assessments` (above). In general, the name will be the name of the method without the trailing \"assessment\". For example, FairnessAssessment -> \"Fairness\". `get_assessment_names` will tell you the names you need.\n",
    "\n",
    "The plan's parameters are passed to each Assessments `init_module` function.\n",
    "\n",
    "Not all assessments *require* a plan, though many can be customized. In the case of \"Performance\" and \"Fairness\", a plan defining a list of metrics should be supplied, though default metrics will be defined for regression/classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83eed596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the metrics that will be used by the Fairness assessment\n",
    "assessment_plan = {\n",
    "    'Fairness': {'metrics': ['precision_score']},\n",
    "    'Performance': {'metrics': ['precision_score']}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3c5b8",
   "metadata": {},
   "source": [
    "### Run Lens\n",
    "\n",
    "Once we have the model and data artifacts, as well as the spec, we can run Lens. By default it will automatically infer which assessments to run, just as we manually did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc76430f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Initializing Assessment (DatasetEquity)\n",
      "INFO:absl:Initializing Assessment (DatasetFairness)\n",
      "INFO:absl:Initializing Assessment (DatasetProfiling)\n",
      "INFO:absl:Initializing Assessment (Fairness) with kwargs: {'metrics': ['precision_score']}\n",
      "INFO:absl:Initializing Assessment (ModelEquity)\n",
      "INFO:absl:Initializing Assessment (Performance) with kwargs: {'metrics': ['precision_score']}\n"
     ]
    }
   ],
   "source": [
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessment_plan=assessment_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5dcbe3",
   "metadata": {},
   "source": [
    "**Getting Assessment Results**\n",
    "\n",
    "To run the assessments with Lens, call `run_assessments`\n",
    "\n",
    "`run_assessments` outputs the results into a dictionary that can be used for further processing. You can also export the data to a json or straight to Credo AI's Governance App by calling `lens.export()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27253691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running assessment: DatasetEquity\n",
      "INFO:absl:Running assessment: DatasetFairness\n",
      "INFO:absl:Running assessment: DatasetProfiling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b90bbad44724264a32be6285c4d49a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running assessment: Fairness\n",
      "INFO:absl:Running assessment: ModelEquity\n",
      "INFO:absl:Running assessment: Performance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['DatasetEquity', 'DatasetFairness', 'DatasetProfiling'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = lens.run_assessments().get_results()\n",
    "results['validation'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75b82a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>sensitive_feature</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision_score</th>\n",
       "      <td>0.020565</td>\n",
       "      <td>SEX</td>\n",
       "      <td>parity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    value sensitive_feature subtype\n",
       "metric_type                                        \n",
       "precision_score  0.020565               SEX  parity"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the fairness results, from the Fairness assessment, run on the validation dataset\n",
    "results['validation_model']['Fairness']['SEX']['fairness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d0dc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualizing assessments\n",
    "\n",
    "Assessments aren't much if you can't visualize them. Lens allows you to visualize your results easily.\n",
    "\n",
    "**Displaying Plots**\n",
    "\n",
    "If you'd like to display the plots in your active jupyter notebook, set `display_results` to True. That's what we did at the top of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bfe1a",
   "metadata": {},
   "source": [
    "**Exporting assessments To Credo AI's Governance App**\n",
    "\n",
    "Finally, the assessments can also be exported to Credo AI's Governance App. Check out the [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/latest/notebooks/governance_integration.html) tutorial for directions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "507a61bde0a0183a71b2d35939f461921273a091e2cc4517af66dd70c4baafc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
