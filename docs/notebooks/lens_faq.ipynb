{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af84a3a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lens FAQ\n",
    "This document answers some of the most common functionality questions you may have.\n",
    "\n",
    "**Find the code**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9783e8bc",
   "metadata": {},
   "source": [
    "Click <a class=\"reference internal\" download=\"\" href=\"../notebooks/lens_faq.ipynb\">here</a> to download this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fa1a3",
   "metadata": {},
   "source": [
    "**Imports and Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3080e2-2ace-4267-b948-44c25ce9c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and data are defined by this script\n",
    "# This is a classification model\n",
    "%run training_script.py\n",
    "import credoai.lens as cl\n",
    "import credoai.evaluators as evl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc258d5-bb22-452e-9cc8-c16408f7358f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do I get my model working with Lens?\n",
    "\n",
    "The first step in using Lens is creating a `Model` in the Lens framework. You will use different subclasses of `Model` depending on your model type. Currently, `ClassificationModel` and `RegressionModel` are defined.\n",
    "\n",
    "A model needs to be passed a \"model_like\" object. This is any object that defines the functions needed by the `Model`. For instance, `ClassificationModel` needs to be passed an object with `predict` function. This object can be a sklearn pipeline, a pytorch model, or any other object that conforms to sklearn's `predict` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8eb583-b3b8-4911-86ad-31010ffecc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.artifacts import ClassificationModel\n",
    "\n",
    "credo_model = ClassificationModel(name='my_model', model_like=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca689590-30aa-4a13-854a-c8bab1946c80",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Using precomputed values**\n",
    "\n",
    "A common use case you may run into is wanting to assess *pre-computed* predictions. You don't need Lens to perform inference, just use the inferences you've already generated for assessment.\n",
    "\n",
    "In order to do so you need to coerce your predictions into a \"model-like\" object. To do so you make use of \"Dummy\" Models. Below is an example for `ClassificationModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab08b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.artifacts import DummyClassifier\n",
    "# precomputed predictions\n",
    "predictions = model.predict(X)\n",
    "probs = model.predict_proba(X)\n",
    "# light wrapping to create the dummy model\n",
    "dummy_model = DummyClassifier(predict_output=predictions, predict_proba_output=probs)\n",
    "# ... which can then be passed to the ClassificationModel\n",
    "credo_model_from_predictions = ClassificationModel(name='my_model_name', model_like=dummy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532d934-754e-4140-85ee-730d0da40c78",
   "metadata": {},
   "source": [
    "## How do I get my datasets working with Lens?\n",
    "\n",
    "`Data` is the equivalent of `Model` for datasets. They can be passed to Lens as \"assessment_data\" (which is the validation data to assess the model against) or as \"training_data\" (which will not be used to evaluate the model, but will be assessed itself).\n",
    "\n",
    "`Data` is subclassed for particular data types. Currently `TabularData` is supported, which you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629a2bdc-d53c-41fa-8b4e-77a4fcf93856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.artifacts import TabularData\n",
    "credo_data = TabularData(name='my_dataset_name',\n",
    "                          X=X_test,\n",
    "                          y=y_test,\n",
    "                          sensitive_features=sensitive_features_test\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26592e-61b4-438b-9f9f-5bfaa2d32df4",
   "metadata": {},
   "source": [
    "A number of things happen under the hood when you set up TabularData. For instance, numpy arrays are transformed into dataframes or series, and (optional) sensitive feature intersections are calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26654d8c-7490-4886-bc5b-37d877de0b51",
   "metadata": {},
   "source": [
    "## How do I get assessment results from Lens?\n",
    "\n",
    "Running assessments isn't very helpful if you can't view them! You can get results by calling `lens.get_results()`\n",
    "\n",
    "All results will be dataframes.\n",
    "\n",
    "**Note**\n",
    "\n",
    "If you want to export the assessments to Credo AI's Governance App, check out the [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/stable/notebooks/governance_integration.html) tutorial for directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53af3cd-22d1-4c45-baba-e435296cbe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-03 12:23:08,785 - lens - INFO - Evaluator Performance added to pipeline. \n",
      "2022-10-03 12:23:09,023 - lens - INFO - Evaluator ModelFairness added to pipeline. Dataset used: assessment_data. Sensitive feature: SEX\n",
      "2022-10-03 12:23:09,024 - lens - INFO - Running evaluation for step: Performance_45ce84\n",
      "2022-10-03 12:23:09,025 - lens - INFO - Running evaluation for step: ModelFairness_a39f56\n"
     ]
    }
   ],
   "source": [
    "lens = cl.Lens(model=credo_model,\n",
    "               assessment_data=credo_data)\n",
    "results = lens \\\n",
    "    .add(evl.Performance(['precision_score', 'recall_score'])) \\\n",
    "    .add(evl.ModelFairness(['precision_score', 'recall_score'])) \\\n",
    "    .run() \\\n",
    "    .get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c62978e-7d52-4bac-81be-0cb81132dee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator: Performance_45ce84\n",
      "\n",
      "              type     value\n",
      "0  precision_score  0.628081\n",
      "1     recall_score  0.360172\n",
      "\n",
      "\n",
      "Evaluator: ModelFairness_a39f56\n",
      "\n",
      "              type     value subtype\n",
      "0  precision_score  0.016322  parity\n",
      "1     recall_score  0.027686  parity\n",
      "\n",
      "      SEX             type     value\n",
      "0  female  precision_score  0.618687\n",
      "1    male  precision_score  0.635009\n",
      "2  female     recall_score  0.344585\n",
      "3    male     recall_score  0.372271\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lens.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f49cf1-4832-42f4-af7e-a12e7679e045",
   "metadata": {},
   "source": [
    "## What metrics are available?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071252a-b96e-4373-88d6-b96ec053ec22",
   "metadata": {},
   "source": [
    "Each assessment has different configuration options, as discused above. Some assessments take a set of metrics as their configuration (the FairnessAssessment and PerformanceAssessment).\n",
    "\n",
    "Many metrics are supported out-of-the-box. These metrics can be referenced by string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d6473e-36d9-4acc-a111-8f730ef1990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY_CLASSIFICATION\n",
      "\taccuracy_score, average_precision_score,\n",
      "\tbalanced_accuracy_score, f1_score, fallout_rate,\n",
      "\tfalse_discovery_rate, false_negative_rate,\n",
      "\tfalse_omission_rate, false_positive_rate, fdr,\n",
      "\tfnr, fpr, hit_rate,\n",
      "\tmatthews_correlation_coefficient, miss_rate,\n",
      "\toverprediction, precision, precision_score,\n",
      "\trecall, recall_score, roc_auc_score,\n",
      "\tselection_rate, sensitivity, specificity, tnr,\n",
      "\ttpr, true_negative_rate, true_positive_rate,\n",
      "\tunderprediction\n",
      "\n",
      "FAIRNESS\n",
      "\tdemographic_parity,\n",
      "\tdemographic_parity_difference,\n",
      "\tdemographic_parity_ratio, disparate_impact,\n",
      "\tequal_opportunity, equal_opportunity_difference,\n",
      "\tequalized_odds, equalized_odds_difference,\n",
      "\tstatistical_parity\n",
      "\n",
      "REGRESSION\n",
      "\tMAE, MSD, MSE, RMSE, d2_tweedie_score,\n",
      "\texplained_variance_score, max_error,\n",
      "\tmean_absolute_error,\n",
      "\tmean_absolute_percentage_error,\n",
      "\tmean_gamma_deviance, mean_pinball_loss,\n",
      "\tmean_poisson_deviance, mean_squared_deviation,\n",
      "\tmean_squared_error, mean_squared_log_error,\n",
      "\tmedian_absolute_error, r2, r2_score, r_squared,\n",
      "\troot_mean_squared_error, target_ks_statistic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all out-of-the-box supported metrics can be accessed by calling list_metrics\n",
    "from credoai.modules import list_metrics\n",
    "metrics = list_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5039c-6d85-452d-98ce-c31c68617f56",
   "metadata": {},
   "source": [
    "Under the hood each metric is wrapped in a `Metric` class. `Metrics` are lightweight wrapper classes that defines a few characteristics of the custom function needed by Lens.\n",
    "\n",
    "This class defines a canonical name for Lens, synonyms, a metric category, the function, and whether the metric takes probabilities or categorical predictions. The metric category defines the expected function signature, as described in `Metric`'s documentation\n",
    "\n",
    "For instance, below is the false positive rate metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ed135b-fda9-4598-b381-e752726cff0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name='false_positive_rate', metric_category='BINARY_CLASSIFICATION', fun=<function false_positive_rate at 0x28ebbd820>, takes_prob=False, equivalent_names={'false_positive_rate', 'fpr', 'fallout_rate'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.modules import BINARY_CLASSIFICATION_METRICS\n",
    "BINARY_CLASSIFICATION_METRICS['false_positive_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc23b24-96d2-44b7-b048-b3d6ca66c0e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do I use my own custom metrics?\n",
    "\n",
    "Custom metrics can be created by using the `Metric` class.  \n",
    "\n",
    "**Example: Confidence Intervals**\n",
    "\n",
    "We will create custom metrics that reflect the lower and upper 95th percentile confidence bound on the true positive rate.\n",
    "\n",
    "Confidence intervals are not supported by default. However, they can be derived for some metrics using the `wilson confidence interval`. We will use a convenience function called `confusion_wilson` returns an array: [lower, upper] bound for metrics like true-positive-rate. \n",
    "\n",
    "Wrapping the wilson function in a `Metric` allows us to use it in Lens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68d83ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.modules.metrics_credoai import confusion_wilson\n",
    "from credoai.modules import Metric\n",
    "\n",
    "# define partial functions for the true positive rate lower bound\n",
    "def lower_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[0]\n",
    "\n",
    "# and upper bound\n",
    "def upper_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[1]\n",
    "\n",
    "# wrap the functions in fairness functions\n",
    "lower_metric = Metric(name = 'lower_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = lower_bound_tpr)\n",
    "\n",
    "upper_metric = Metric(name = 'upper_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = upper_bound_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a44df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-03 12:23:09,289 - lens - INFO - Evaluator Performance added to pipeline. \n",
      "2022-10-03 12:23:09,290 - lens - INFO - Running evaluation for step: Performance_1b1bd0\n",
      "Evaluator: Performance_1b1bd0\n",
      "\n",
      "              type     value\n",
      "0  lower_bound_tpr  0.337201\n",
      "1              tpr  0.360172\n",
      "2  upper_bound_tpr  0.383802\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run Lens as normal with custom metrics\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               assessment_data=credo_data)\n",
    "lens.add(evl.Performance([lower_metric, 'tpr', upper_metric]))\n",
    "lens.run().print_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "507a61bde0a0183a71b2d35939f461921273a091e2cc4517af66dd70c4baafc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
