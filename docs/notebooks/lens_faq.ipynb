{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af84a3a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lens FAQ\n",
    "This document answers some of the most common functionality questions you may have.\n",
    "\n",
    "**A note on customization**\n",
    "\n",
    "Lens strives to give you sensible defaults and automatically do the proper assessments whenever possible. However, there are many times where you'll want to change, select, or extend the functionality. Lens is intended to be an _extensible framework_ that can accomodate your own analysis plan.\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "This notebook can be found on [github](https://github.com/credo-ai/credoai_lens/blob/develop/docs/notebooks/lens_faq.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b71ce-6b7a-4dbc-a10d-c30dcb256e18",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3080e2-2ace-4267-b948-44c25ce9c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and df are defined by this script\n",
    "%run training_script.py\n",
    "import credoai.lens as cl\n",
    "\n",
    "# specify the metrics that will be used by the Fairness and Performance assessment\n",
    "metrics = ['precision_score', 'recall_score', 'equal_opportunity']\n",
    "assessment_plan = {'Fairness': {'metrics': metrics},\n",
    "                   'Performance': {'metrics': metrics}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc258d5-bb22-452e-9cc8-c16408f7358f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do I get my model working with Lens?\n",
    "\n",
    "The first step in using Lens is creating a `CredoModel`. Most kinds of models can be wrapped in a `CredoModel` for assessment. `CredoModel`'s primary attribute is its `config` which is a dictionary reflecting its functionality. This config is what determines how the model will be interacted with, and thus which assessments can be used.\n",
    "\n",
    "The simplest case happens when your model's functionality can be inferred by Lens, which is used to define the `config`. Below is an example using a scikitlearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e8eb583-b3b8-4911-86ad-31010ffecc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frameworks: ('sklearn', 'xgboost')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predict': <bound method ForestClassifier.predict of RandomForestClassifier()>,\n",
       " 'predict_proba': <function credoai.artifacts.CredoModel._sklearn_style_config.<locals>.predict_proba(X)>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('frameworks:', cl.CredoModel.supported_frameworks())\n",
    "credo_model = cl.CredoModel(name='my_model',\n",
    "                            model=model)\n",
    "# configuration automatically inferred from model\n",
    "credo_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ebb3b-0b8f-4e46-a09e-c3dc61375754",
   "metadata": {},
   "source": [
    "**Defining the config manually**\n",
    "\n",
    "While this config is inferred when working with a supported model (e.g., sklearn, above), you can also define it manually. For instance, you can define the same CredoModel like below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f5f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'predict_proba': model.predict_proba,\n",
    "          'predict':  model.predict}\n",
    "credo_model = cl.CredoModel(name='my_model',\n",
    "                            model_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc21bb-3d9d-49b4-ba21-2c7c3e4e07cc",
   "metadata": {},
   "source": [
    "This is a much more generic approach. A CredoModel is just a collection of functions that conform to some function spec. For instance, the \"predict\" function above must conform to the function signature of the `predict` function used in sklearn models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca689590-30aa-4a13-854a-c8bab1946c80",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Using precomputed values**\n",
    "\n",
    "A common use case you may run into is wanting to assess *pre-computed* predictions. You don't need Lens to perform inference, just use the inferences you've already generated for assessment.\n",
    "\n",
    "Below is an example of such a case using the same example. Note that the `predict` still needs to take in an `X` variable to maintain the appropriate function signature. In this case, however, X is completely ignored and the predictions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab08b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precomputed predictions\n",
    "predictions = model.predict(X)\n",
    "probs = model.predict_proba(X)\n",
    "# light wrapping\n",
    "config = {'predict': lambda X: predictions,\n",
    "          'predict_proba': lambda X: probs}\n",
    "credo_model = cl.CredoModel(name='my_model_name',\n",
    "                            model_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532d934-754e-4140-85ee-730d0da40c78",
   "metadata": {},
   "source": [
    "## How do I get my datasets working with Lens?\n",
    "\n",
    "`CredoData` are the equivalent of `CredoModels` for datasets. They can be passed to Lens as \"data\" (which is the validation data to assess the model against) or as \"training_data\" (which will not be used to evaluate the model, but will be assessed itself).\n",
    "\n",
    "CredoData expects a dataframe that includes all the features used to train the model (and to potentially call the model's `predict` function), as well as the target. Optionally, a sensitive feature key can be passed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629a2bdc-d53c-41fa-8b4e-77a4fcf93856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model and data artifacts\n",
    "credo_data = cl.CredoData(name='my_dataset_name',\n",
    "                          data=df,\n",
    "                          sensitive_feature_keys=['SEX'],\n",
    "                          label_key='target'\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26654d8c-7490-4886-bc5b-37d877de0b51",
   "metadata": {},
   "source": [
    "## How do I get assessment results from Lens?\n",
    "\n",
    "Running assessments isn't very helpful if you can't view them! \n",
    "\n",
    "* You can get results by calling `lens.get_results()`\n",
    "* You can visualize results by calling `lens.display_results()`\n",
    "\n",
    "All results will be dictionaries or pandas objects.\n",
    "\n",
    "**Note**\n",
    "\n",
    "If you want to export the assessments to Credo AI's Governance App, check out the [Connecting with Governance App](https://credoai-lens.readthedocs.io/en/latest/notebooks/governance_integration.html) tutorial for directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53af3cd-22d1-4c45-baba-e435296cbe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:fairness metric, equal_opportunity, unused by PerformanceModule\n",
      "INFO:absl:Running assessment: DatasetFairness\n",
      "INFO:absl:Running assessment: DatasetProfiling\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6a569a1e34442bbffbda345d8b8919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running assessment: Fairness\n",
      "INFO:absl:Running assessment: Performance\n"
     ]
    }
   ],
   "source": [
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessment_plan=assessment_plan).run_assessments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c62978e-7d52-4bac-81be-0cb81132dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lens.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32bad1-d0ec-499a-9c80-57e0fa6af545",
   "metadata": {},
   "source": [
    "Results are organized in a hierarchical dictionary. The first level describes the artifacts (data and/or models used for the assessment). The three possible artifcacts are:\n",
    "\n",
    "1. training (for training data)\n",
    "2. validation (for validation data)\n",
    "3. model\n",
    "\n",
    "The first level will have keys like \"validation_model\", or \"training_validation_model\", indicating which artifacts are related to those artifacts. In this case, we didn't pass a training dataset, and no assessments were run only on the model, so we have assessments run on the validation dataset, or on the validation dataset and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac39092-b977-4dce-a264-9e160e8a606e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['validation', 'validation_model'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f104178-80cb-4845-9cca-035655b7363f",
   "metadata": {},
   "source": [
    "The next level are the assessment names. In this case, the \"Fairness\" and \"Performance\" assessments were automatically run on the validation dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a8b9af-ee79-4176-acce-a74ea6245f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Fairness', 'Performance'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['validation_model'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2386644b-6728-40bf-b828-f9ef496a9d67",
   "metadata": {},
   "source": [
    "Finally, there are keys related to the different results created by each assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5ba949-6917-46f4-9e2e-433dc29eaf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SEX-disaggregated_performance', 'fairness'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['validation_model']['Fairness'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811e3d51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How can I choose which assessments to run?\n",
    "Lens has a number of assessments available, each of which works with different kinds of models or datasets. **By default, Lens will automatically run every assessment that has its prerequesites met.** \n",
    "\n",
    "However, you can instead specify a list of assessments and Lens will only select from those. Even when you select assessments, Lens will only run the ones that work with your model and data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bfda748-45ca-4243-96bc-18b37afaa26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying a set of assessments to use\n",
    "\n",
    "from credoai.assessment import FairnessAssessment, DatasetFairnessAssessment\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessment_plan=assessment_plan,\n",
    "               assessments=[FairnessAssessment, # <- new argument\n",
    "                            DatasetFairnessAssessment] \n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422da29-22e8-4947-80c0-daa89d07dbc6",
   "metadata": {},
   "source": [
    "### List all assessments and their names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07c560-6ff2-405b-9573-cb3363ab7f52",
   "metadata": {},
   "source": [
    "To list all assessments that are available, as well as their names (needed for creating assessment plans), use the helper function `get_assessment_names`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926d1681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetFairnessAssessment': 'DatasetFairness',\n",
       " 'DatasetProfilingAssessment': 'DatasetProfiling',\n",
       " 'FairnessAssessment': 'Fairness',\n",
       " 'NLPEmbeddingBiasAssessment': 'NLPEmbeddingBias',\n",
       " 'NLPGeneratorAssessment': 'NLPGenerator',\n",
       " 'PerformanceAssessment': 'Performance',\n",
       " 'PrivacyAssessment': 'Privacy',\n",
       " 'SecurityAssessment': 'Security'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment.utils import get_assessment_names\n",
    "get_assessment_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a3fe6a-64fb-42a5-9ece-f4d5c77696c3",
   "metadata": {},
   "source": [
    "## How do I get documentation on the assessments I am running?\n",
    "\n",
    "Assessment documentation is somewhat complex. Assessments wrap modules, which themselves have documentation. Normally, you don't have to worry about the module itself, except if you are creating your own assessments or want to use the modules directly. The main exception here is if you want to configure how you run the assessment (see this [section](#How-do-I-configure-my-assessments?)).\n",
    "\n",
    "For the assessments, you may be interested in what parameters you can pass to their initialization (which is passed using the `assessment_plan` parameter when running Lens).\n",
    "\n",
    "Below are a number of aspects of an assessment you may need to query. The next few sections expand on these aspects of assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c4a792b-3e2c-4bbf-8923-9021bfea0b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this section to run!\n",
    "# # Different aspects of documentation you may be interested in\n",
    "# # These are all for the fairness assessment\n",
    "\n",
    "# # what parameters can be passed to the initialization?\n",
    "# FairnessAssessment.init_module?\n",
    "\n",
    "# # what requirements are needed? \n",
    "# # (This is normally included in the assessments base documentation)\n",
    "# FairnessAssessment().get_requirements()\n",
    "\n",
    "# # what does the module require? \n",
    "# # This is often similar to the parameters passed to assessment initialization\n",
    "# FairnessAssessment().module?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8733f6-64e3-453b-bfbb-7158cc54c33f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding assessment requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d005a51-f2d8-4c15-820d-5d5f1d18de52",
   "metadata": {},
   "source": [
    "The prerequesities can be queried by calling the `get_requirements` function on an assessment. These indicate the set of features or functions your model and data must instantiate in order for the assessment to be run. \n",
    "\n",
    "These requirements are specific for the model and the data. Each requirement is either a single requirements or a tuple. If a tuple, only one of the requirements within the tuple must be met. For instance, the FairnessAssessment needs *either* `predict_proba` OR `predict`. See `credoai.assessments.credo_assessment.AssessmentRequirements` for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7137eee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_requirements': [('predict_proba', 'predict')],\n",
       " 'data_requirements': ['X', 'y', 'sensitive_features'],\n",
       " 'training_data_requirements': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import FairnessAssessment\n",
    "assessment = FairnessAssessment()\n",
    "assessment.get_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ed24e-7bf7-4dea-af1d-bfbd13b00ed0",
   "metadata": {},
   "source": [
    "You can also get the requirements for all assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c803c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatasetFairnessAssessment': {'model_requirements': [],\n",
       "  'data_requirements': ['X', 'y', 'sensitive_features'],\n",
       "  'training_data_requirements': []},\n",
       " 'DatasetProfilingAssessment': {'model_requirements': [],\n",
       "  'data_requirements': ['X', 'y'],\n",
       "  'training_data_requirements': []},\n",
       " 'FairnessAssessment': {'model_requirements': [('predict_proba', 'predict')],\n",
       "  'data_requirements': ['X', 'y', 'sensitive_features'],\n",
       "  'training_data_requirements': []},\n",
       " 'NLPEmbeddingBiasAssessment': {'model_requirements': ['embedding_fun'],\n",
       "  'data_requirements': [],\n",
       "  'training_data_requirements': []},\n",
       " 'NLPGeneratorAssessment': {'model_requirements': ['generator_fun'],\n",
       "  'data_requirements': [],\n",
       "  'training_data_requirements': []},\n",
       " 'PerformanceAssessment': {'model_requirements': [('predict_proba',\n",
       "    'predict')],\n",
       "  'data_requirements': ['X', 'y'],\n",
       "  'training_data_requirements': []},\n",
       " 'PrivacyAssessment': {'model_requirements': ['predict'],\n",
       "  'data_requirements': ['X', 'y'],\n",
       "  'training_data_requirements': ['X', 'y']},\n",
       " 'SecurityAssessment': {'model_requirements': ['predict'],\n",
       "  'data_requirements': ['X', 'y'],\n",
       "  'training_data_requirements': ['X', 'y']}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import get_assessment_requirements\n",
    "get_assessment_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2246bd48-6793-477c-901a-410f9f69845d",
   "metadata": {},
   "source": [
    "You can see which assessments are usable for a particular combination of artifacts by passing them to the helper function `get_usable_assessments`. Lens does this under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "955164a1-aee7-4793-a4f0-49d0d2c7b1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fairness': <credoai.assessment.assessments.FairnessAssessment at 0x290cd4c40>,\n",
       " 'Performance': <credoai.assessment.assessments.PerformanceAssessment at 0x290cd4220>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import get_usable_assessments\n",
    "get_usable_assessments(credo_model=credo_model, \n",
    "                       credo_data=credo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b510e-42c6-4ff7-aeca-7eaccc0fcf3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do I configure my assessments?\n",
    "\n",
    "Now that we can select assessments, how about configuring them? There are two places where assessments can be configured: \n",
    "1. when their underlying module is initialized \n",
    "2. when they are ran (which runs the underlying module).\n",
    "\n",
    "### Configuring initialization\n",
    "To configure the module at initialization we use something we've already seen before - the `assessment plan`! The parameters that can be passed at this stage are the same parameters passed to the assessment's `init_module` function. \n",
    "\n",
    "The only one we've seen so far are the \"metrics\" argument that can be passed to the PerformanceAssessment and FairnessAssessment, but other assessments may be configured in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a575093-f318-4c79-9b70-a50f54fd4343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view the arguments that can be passed to an assessment's initialization, see the documentation for its \"init_module\" function\n",
    "# Uncomment below to see this for the Performance Assessment\n",
    "\n",
    "# assessment = PerformanceAssessment()\n",
    "# assessment.init_module?\n",
    "\n",
    "assessment_plan = {'Performance': {'metrics': metrics}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc1a24-b8ce-4608-9942-9ce025622b32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuring runtime\n",
    "\n",
    "The other way of parameterizing the assessments is by passing arguments to the assessment's `run` function. These kwargs are passed to `lens.run_assessments`, which are, in turn passed to the assessment's initialized module.\n",
    "\n",
    "For instance, the `Fairness` assessment initializes `mod.Fairness`, whose `run` argument can take a `method` parameter which controls how fairness scores are calculated. The default is \"between_groups\", but we can change it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a920f75d-27da-4b2f-a0fc-03dc2c45c0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running assessment: DatasetFairness\n",
      "INFO:absl:Running assessment: Fairness\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<credoai.lens.Lens at 0x290cc3550>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To view the arguments that can be passed to an assessment's run function, view the underlying module's run method\n",
    "# Uncomment below to see this for the Fairness Assessment\n",
    "\n",
    "# assessment = FairnessAssessment()\n",
    "# assessment.module.run?\n",
    "\n",
    "run_kwargs = {'Fairness': {'method': 'to_overall'}}\n",
    "lens.run_assessments(assessment_kwargs = run_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f49cf1-4832-42f4-af7e-a12e7679e045",
   "metadata": {},
   "source": [
    "## What metrics are available?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071252a-b96e-4373-88d6-b96ec053ec22",
   "metadata": {},
   "source": [
    "Each assessment has different configuration options, as discused above. Some assessments take a set of metrics as their configuration (the FairnessAssessment and PerformanceAssessment).\n",
    "\n",
    "Many metrics are supported out-of-the-box. These metrics can be referenced by string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4d6473e-36d9-4acc-a111-8f730ef1990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY_CLASSIFICATION\n",
      "\taccuracy_score, average_precision_score,\n",
      "\tbalanced_accuracy_score, f1_score, fallout_rate,\n",
      "\tfalse_discovery_rate, false_negative_rate,\n",
      "\tfalse_omission_rate, false_positive_rate, fdr,\n",
      "\tfnr, fpr, hit_rate,\n",
      "\tmatthews_correlation_coefficient, miss_rate,\n",
      "\toverprediction, precision, precision_score,\n",
      "\trecall, recall_score, roc_auc_score,\n",
      "\tselection_rate, sensitivity, specificity, tnr,\n",
      "\ttpr, true_negative_rate, true_positive_rate,\n",
      "\tunderprediction\n",
      "\n",
      "FAIRNESS\n",
      "\tdemographic_parity,\n",
      "\tdemographic_parity_difference,\n",
      "\tdemographic_parity_ratio, disparate_impact,\n",
      "\tequal_opportunity, equal_opportunity_difference,\n",
      "\tequalized_odds, equalized_odds_difference,\n",
      "\tstatistical_parity\n",
      "\n",
      "REGRESSION\n",
      "\tMAE, MSD, MSE, RMSE, d2_tweedie_score,\n",
      "\texplained_variance_score, max_error,\n",
      "\tmean_absolute_error,\n",
      "\tmean_absolute_percentage_error,\n",
      "\tmean_gamma_deviance, mean_pinball_loss,\n",
      "\tmean_poisson_deviance, mean_squared_deviation,\n",
      "\tmean_squared_error, mean_squared_log_error,\n",
      "\tmedian_absolute_error, r2, r2_score, r_squared,\n",
      "\troot_mean_squared_error, target_ks_statistic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all out-of-the-box supported metrics can be accessed by calling list_metrics\n",
    "from credoai.metrics import list_metrics\n",
    "metrics = list_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5039c-6d85-452d-98ce-c31c68617f56",
   "metadata": {},
   "source": [
    "Under the hood each metric is wrapped in a `Metric` class. `Metrics` are lightweight wrapper classes that defines a few characteristics of the custom function needed by Lens.\n",
    "\n",
    "This class defines a canonical name for Lens, synonyms, a metric category, the function, and whether the metric takes probabilities or categorical predictions. The metric category defines the expected function signature, as described in `Metric`'s documentation\n",
    "\n",
    "For instance, below is the false positive rate metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36ed135b-fda9-4598-b381-e752726cff0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name='false_positive_rate', metric_category='BINARY_CLASSIFICATION', fun=<function false_positive_rate at 0x28ca64550>, takes_prob=False, equivalent_names={'fallout_rate', 'false_positive_rate', 'fpr'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.metrics import BINARY_CLASSIFICATION_METRICS\n",
    "BINARY_CLASSIFICATION_METRICS['false_positive_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc23b24-96d2-44b7-b048-b3d6ca66c0e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How do I use my own custom metrics?\n",
    "\n",
    "Custom metrics can be created by using the `Metric` class.  \n",
    "\n",
    "**Example: Confidence Intervals**\n",
    "\n",
    "We will create custom metrics that reflect the lower and upper 95th percentile confidence bound on the true positive rate.\n",
    "\n",
    "Confidence intervals are not supported by default. However, they can be derived for some metrics using the `wilson confidence interval`. We will use a convenience function called `confusion_wilson` returns an array: [lower, upper] bound for metrics like true-positive-rate. \n",
    "\n",
    "Wrapping the wilson function in a `Metric` allows us to use it in Lens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68d83ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.metrics.credoai_metrics import confusion_wilson\n",
    "from credoai.metrics import Metric\n",
    "\n",
    "# define partial functions for the true positive rate lower bound\n",
    "def lower_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[0]\n",
    "\n",
    "# and upper bound\n",
    "def upper_bound_tpr(y_true, y_pred):\n",
    "    return confusion_wilson(y_true, y_pred, metric='true_positive_rate', confidence=0.95)[1]\n",
    "\n",
    "# wrap the functions in fairness functions\n",
    "lower_metric = Metric(name = 'lower_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = lower_bound_tpr)\n",
    "\n",
    "upper_metric = Metric(name = 'upper_bound_tpr', \n",
    "                      metric_category = \"binary_classification\",\n",
    "                      fun = upper_bound_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83a44df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running assessment: Performance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lower_bound_tpr</th>\n",
       "      <td>0.995349</td>\n",
       "      <td>overall_performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tpr</th>\n",
       "      <td>0.996986</td>\n",
       "      <td>overall_performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_bound_tpr</th>\n",
       "      <td>0.998048</td>\n",
       "      <td>overall_performance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    value              subtype\n",
       "lower_bound_tpr  0.995349  overall_performance\n",
       "tpr              0.996986  overall_performance\n",
       "upper_bound_tpr  0.998048  overall_performance"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.assessment import PerformanceAssessment\n",
    "\n",
    "assessment_plan = {'Performance': {'metrics': [lower_metric, 'tpr', upper_metric]}}\n",
    "lens = cl.Lens(model=credo_model,\n",
    "               data=credo_data,\n",
    "               assessments=[PerformanceAssessment],\n",
    "              assessment_plan=assessment_plan)\n",
    "lens.run_assessments().get_results()['validation_model']['Performance']['overall_performance']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d7eb8a87bb83596f4cd5aeb66d856dad2a9bb65fe804cea051250e36746a46f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
