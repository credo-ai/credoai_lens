{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credo AI Platform Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a comprehensive overview of how Credo AI Lens interacts with the Credo AI Platform.\n",
    "\n",
    "Connecting Lens to the Platform requires that you have already created a Use-Case, and a policy pack defining how the associated models and data should be assessed.\n",
    "\n",
    "**Find the code**\n",
    "\n",
    "Download this notebook from [github](https://github.com/credo-ai/credoai_lens/blob/main/docs/notebooks/platform_integration.ipynb).\n",
    "\n",
    "If you want to just get the boilerplate code, see [example in this script](https://raw.githubusercontent.com/credo-ai/credoai_lens/develop/docs/example_code.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "A list of useful keywords:  \n",
    "\n",
    "- **Credo AI Platform**: Also referred to as simply \"Platform\". The central AI governance/reporting Platform, found at [https://app.credo.ai/](https://app.credo.ai/)\n",
    "\n",
    "- **credoconfig**: configuration file to be copied in the user's home folder\n",
    "\n",
    "- **use_case_name**: The name of your Use Case as it is registered on Credo AI Platform\n",
    "\n",
    "- **policy_pack**: A set of governance controls that a Use Case needs to satisfy. A Use Case can have multiple policy packs applied to it.\n",
    "\n",
    "- **policy_pack_key**: A unique identifier for a policy pack (where do we get this?)\n",
    "\n",
    "- **assessment_plan**: A set of evaluations that `Lens` needs to run in order to satisfy a policy pack requirements.\n",
    "\n",
    "- **assessment_plan_url**: The link to the assessment plan, this is generated in the Platform and used to download the assessment plan in the Governance object. See example below.\n",
    "\n",
    "- **evidence**: The output of a `Lens` evaluation, formatted specifically to be uploaded to the platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup API Connection with the Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a config file\n",
    "This file contains all the necessary information to connect Lens to the Credo AI Platform.  \n",
    "\n",
    "To generate the config file, once you logged into the platform, click on your name icon (top left) and follow:  \n",
    "\n",
    "`My Settings -> Tokens -> Plus Sign -> Generate`\n",
    "\n",
    "Immediately after generating the token, you will be given the possibility to download the config file.\n",
    "\n",
    "The default location/file name Lens is expecting is `~/.credoconfig`, where `~` is your home folder. You can specify any other location when you are initializing the `Governance` object (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your ML environment ready\n",
    " In this tutorial we will emulate the modeling phase by running a quick script. This script loads a dataset, splits it into training and testing, and fits a model. You can see the full script [here](https://github.com/credo-ai/credoai_lens/blob/release/1.0.0/docs/notebooks/training_script.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.governance import Governance\n",
    "from credoai.lens import Lens\n",
    "from credoai.artifacts import ClassificationModel, TabularData\n",
    "from credoai.datasets import fetch_credit_model\n",
    "\n",
    "# model and data are defined by this script\n",
    "X_test, y_test, sensitive_features_test, model = fetch_credit_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Governance in 5 Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up governance\n",
    "gov = Governance()  # Specify config_path if your config file is not the default one: ~/.credoconfig\n",
    "url = 'your assessment url'\n",
    "gov.register(assessment_plan_url=url)\n",
    "\n",
    "# set up model and data artifacts\n",
    "credo_model = ClassificationModel(name=\"credit_default_classifier\", model_like=model)\n",
    "\n",
    "credo_data = TabularData(\n",
    "    name=\"UCI-credit-default\",\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    sensitive_features=sensitive_features_test,\n",
    ")\n",
    "\n",
    "# Initialization of the Lens object\n",
    "lens = Lens(model=credo_model, assessment_data=credo_data, governance=gov)\n",
    "\n",
    "# Run Lens\n",
    "lens.run()\n",
    "# Update governance object with newly created evidence\n",
    "lens.send_to_governance()\n",
    "# Export to platform\n",
    "gov.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Down the Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Governance\n",
    "In this phase we create the Governance object with all the necessary information required to run a battery of assessments defined by your use case and policy pack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov = Governance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing the `Governance` object, we need to register an assessment plan. This can be achieved in three ways:  \n",
    "\n",
    "1. Using the `assessment_plan_url`. In the Platform: your_use_case -> Reports -> Your_policy_pack -> **ASSESSMENT PLAN** -> **Copy URL**\n",
    "2. Using the assessment plan json. Follow steps in 1., but click on **Download** as the last step. (more details in [Air Gap Environment](##Air-Gap-Environment))\n",
    "3. Using use_case_name and policy pack key as they are defined in the Platform\n",
    "\n",
    "In the tutorial we will follow 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'your_assessment_url'\n",
    "gov.register(assessment_plan_url=url)\n",
    "\n",
    "# Alternatively method 2.\n",
    "# gov.register(use_case_name='your_use_case_name', policy_pack_key='your_policy_pack_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Lens\n",
    "This part is very similar to a standard `Lens` run. See the [quickstart notebook](https://credoai-lens.readthedocs.io/en/stable/notebooks/quickstart.html) for more general information on `Lens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credo_model = ClassificationModel(\"credit_default_classifier\", model)\n",
    "\n",
    "test_data = TabularData(\n",
    "    name=\"UCI-credit-default-test\",\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    sensitive_features=sensitive_features_test,\n",
    ")\n",
    "\n",
    "lens = Lens(\n",
    "    model=credo_model,\n",
    "    assessment_data=test_data,\n",
    "    governance=gov\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `Governance` is included in the initialization of Lens, there is no need to specify which evaluators to include in the pipeline. Under the hood, `Lens` converts the assessment plan contained in `gov` into a list of evaluators.\n",
    "\n",
    "A simple `lens.run()` is sufficient to run all the required assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the execution proceeded according to expectations, the evidence, i.e., the results of the assessments, can be sent to the `Governance` object and finally exported to the platform. In the example below, while sending the evidence to governance, we opted to overwrite any prior evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens.send_to_governance(overwrite_governance=True)\n",
    "gov.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! With just a few lines of code, a set of evidences documenting compliance to a set of controls defined by a policy pack has been uploaded to the Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air Gap Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cannot interact with Credo AI Platform via the internet, this section is for you!\n",
    "\n",
    "Instead of Lens automating the communication between your assessment environment and governance, you will instead have to download an assessment plan, provide it to `Governance`, run `Lens` and finally upload the evidence(assessments results) to the platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Getting the assessment plan\n",
    "The asset that you have to get _from_ the governance app is the assessment plan. This can be found in:   \n",
    "\n",
    "your_use_case -> Reports -> Your_policy_pack -> **ASSESSMENT PLAN** -> **Download**  \n",
    "\n",
    "Once you download the assessments plan, you can pass it to the Governance object, see code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov.register(assessment_plan_file='your_assessment_file.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Results\n",
    "\n",
    "Lens can export evidence to file as a json object. This is generally useful if you want to store your results locally, but particularly useful if you are in an air gapped environment where you cannot access Credo AI's Governance Platform directly from your development environment.\n",
    "\n",
    "Doing this only requires a one line change in our current flow. We change the `export` argument from `Governance` to a local path:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the evidence locally\n",
    "gov.export('evidence_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above will create a folder that you specified, if it doesn't exist. It will then create a json file with the assessment results, which can be uploaded to Credo AI's Governance Platform. This json file can be uploaded by going to:  \n",
    "\n",
    "your_use_case -> Reports -> Your_policy_pack -> **UPLOAD LENS EVIDENCE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env2': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "507a61bde0a0183a71b2d35939f461921273a091e2cc4517af66dd70c4baafc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
