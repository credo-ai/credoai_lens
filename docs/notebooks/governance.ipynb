{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "CredoGovernance is responsible to interact with Credo AI app(server). It gets evidece requirements through assessmnet plan, and uploads evidences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-15 23:51:13,822 - lens - INFO - Successfully registered with 5 evidence requirements\n",
      "2022-09-15 23:51:13,823 - lens - INFO - Uploading 1 evidences.. for use_case_id=64YUaLWSviHgibJaRWr3ZE policy_pack_id=NYCE+1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.governance.governance import CredoGovernance\n",
    "from credoai.evidence.evidence import Metric\n",
    "\n",
    "gov = CredoGovernance()\n",
    "\n",
    "# Getting assessment plan\n",
    "url = \"http://localhost:4000/api/v2/credoai/use_cases/64YUaLWSviHgibJaRWr3ZE/assessment_plans/NYCE+1\"\n",
    "gov.register(assessment_plan_url=url)\n",
    "\n",
    "# evidence_requirements = gov.get_evidence_requirements()\n",
    "# evidences = gather_evidences(evidence_requirements)\n",
    "\n",
    "# Adding evidences\n",
    "gov.set_evidences([])\n",
    "\n",
    "evidence = Metric(\n",
    "    type=\"equal_opportunity_difference\",\n",
    "    value=0.2,\n",
    "    model_name=\"superich detector\",\n",
    "    dataset_name=\"account data\"\n",
    ")\n",
    "\n",
    "gov.add_evidences([evidence])\n",
    "\n",
    "# Upload evidences\n",
    "gov.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing CredoGovernance with default configuration\n",
    "It reads configuration from `~/.credoconfig` file by default. You can download it from Credo AI governance app[http://app.credo.ai], My settings > Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.governance.governance import CredoGovernance\n",
    "\n",
    "gov = CredoGovernance()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing CredoGovernance with configuration file\n",
    "You can also specify configuration file to initialize `credo_api_client`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credoai.governance.credo_api_client import CredoApiClient, CredoApiConfig\n",
    "\n",
    "config = CredoApiConfig()\n",
    "config.load_config(\"config_file_name\")\n",
    "client = CredoApiClient(config=config)\n",
    "\n",
    "gov = CredoGovernance(credo_api_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering : getting evidence requirements of assessment plans\n",
    "There are 3 ways of registering\n",
    "1. With assessment plan URL. You might receive the email with this URL. You can also get this URL from Credo AI app, Use case > Report\n",
    "2. With Use case name and Policy pack key. \n",
    "3. With JSON string(or file) of assessmet plan. It is useful for air-gap condition. You can get this file from Credo AI app, Use case > Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-15 23:11:30,882 - lens - INFO - Successfully registered with 5 evidence requirements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<credoai.evidence.evidence_requirement.EvidenceRequirement at 0x169e245e0>,\n",
       " <credoai.evidence.evidence_requirement.EvidenceRequirement at 0x169e244f0>,\n",
       " <credoai.evidence.evidence_requirement.EvidenceRequirement at 0x169e24640>,\n",
       " <credoai.evidence.evidence_requirement.EvidenceRequirement at 0x169e24070>,\n",
       " <credoai.evidence.evidence_requirement.EvidenceRequirement at 0x169e242e0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With assesmsnet plan url\n",
    "url = \"http://localhost:4000/api/v2/credoai/use_cases/64YUaLWSviHgibJaRWr3ZE/assessment_plans/NYCE+1\"\n",
    "gov.register(assessment_plan_url=url)\n",
    "gov.get_evidence_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Use case name and Policy pack key\n",
    "gov.register(use_case_name=\"Keyboard Sensitivity\", policy_pack_key=\"NYCE\")\n",
    "gov.get_evidence_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With JSON file\n",
    "gov.register(assessment_plan_file=\"/Users/chang/Downloads/assessment-plan-NYCE+1.json\")\n",
    "gov.get_evidence_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading evidences\n",
    "After gathering evidences by evidence requirements, it is time to upload evidences. It will upload evidences to Credo AI app's evidence store of use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'metric',\n",
       "  'label': {'metric_type': 'equal_opportunity_difference'},\n",
       "  'data': {'value': 0.2,\n",
       "   'confidence_interval': None,\n",
       "   'confidence_level': None},\n",
       "  'generated_at': '2022-09-15T14:50:39.783678',\n",
       "  'metadata': {'model_name': 'superich detector',\n",
       "   'dataset_name': 'account data'}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from credoai.evidence.evidence import Metric\n",
    "\n",
    "gov.set_evidences([])\n",
    "\n",
    "evidence = Metric(\n",
    "    type=\"equal_opportunity_difference\",\n",
    "    value=0.2,\n",
    "    model_name=\"superich detector\",\n",
    "    dataset_name=\"account data\"\n",
    ")\n",
    "\n",
    "gov.add_evidences([evidence])\n",
    "list(map(lambda e: e.struct(), gov._evidences))\n",
    "\n",
    "gov.export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
