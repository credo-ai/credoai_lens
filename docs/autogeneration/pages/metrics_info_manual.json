{
    "metric_name": {
        "0": "accuracy_score",
        "1": "AttributeInference",
        "2": "average_precision_score",
        "3": "balanced_accuracy_score",
        "4": "d2_tweedie_score",
        "5": "demographic_parity_difference",
        "6": "demographic_parity_ratio",
        "7": "extraction_attack_score",
        "8": "evasion_attack_score",
        "9": "equalized_odds_difference",
        "10": "equal_opportunity_difference",
        "11": "explained_variance_score",
        "12": "f1_score",
        "13": "false_discovery_rate",
        "14": "false_negative_rate",
        "15": "false_omission_rate",
        "16": "false_positive_rate",
        "17": "matthews_correlation_coefficient",
        "18": "max_error",
        "19": "mean_absolute_error",
        "20": "mean_absolute_percentage_error",
        "21": "mean_gamma_deviance",
        "22": "mean_pinball_loss",
        "23": "mean_poisson_deviance",
        "24": "mean_squared_error",
        "25": "mean_squared_log_error",
        "26": "median_absolute_error",
        "27": "MembershipInference",
        "28": "overprediction",
        "29": "precision_score",
        "30": "r2_score",
        "31": "roc_auc_score",
        "32": "root_mean_squared_error",
        "33": "selection_rate",
        "34": "sensitive_feature_prediction_score",
        "35": "target_ks_statistic",
        "36": "true_negative_rate",
        "37": "true_positive_rate",
        "38": "underprediction"
    },
    "description": {
        "0": "Accuracy is the fraction of predictions that a classification model got right. This metric is not robust to class imbalance.\n\nThe best value is 1 and the worst value is 0.\n\n.. math::\n\n   \\text{Accuracy} = \\frac{Correct \\ Predictions}{Total \\ Number \\ of \\ Examples}",
        "1": "Attribute inference attack occurs when an attacker attempts to learn the attacked feature from the rest of the features.\n\nAttribute inference attack score is the accuracy score of this binary classification task on a balanced dataset.\n\nThe best value is 0 and the worst value is 1.",
        "2": "Average precision summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight.",
        "3": "The balanced accuracy in classification problems is used to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.\n\nThe best value is 1 and the worst value is 0.",
        "4": ":math:`D^2`  regression score is percentage of `Tweedie deviance <https:\/\/en.wikipedia.org\/wiki\/Tweedie_distribution#The_Tweedie_deviance>`__ explained.",
        "5": "Demographic parity difference is a parity metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.\n\nDemographic parity difference should be ideally 0.",
        "6": "Demographic parity ratio is a parity metric that is satisfied if the results of a model's classification are not dependent on a given sensitive attribute.\n\nDemographic parity ratio should be ideally 1.",
        "7": "Model extraction attack occurs when an attacker with black-box access to a model attempts to train a substitute model of it.\n\nModel extraction attack score is the accuracy of the thieved model divided by the accuracy of the victim model, corrected for chance.\n\nThe best value is 0 and the worst value is 1.",
        "8": "Model evasion attack occurs when an attacker with black-box access to a model attempts to create minimally-perturbed samples that get misclassified by the model.\n\nModel evasion attack score the success rate of this attack.\n\nThe best value is 0 and the worst value is 1.",
        "9": "The equalized odds difference of 0 means that all groups have the same true positive, true negative, false positive, and false negative rates.",
        "10": "The equalized odds difference is equivalent to the `true_positive_rate_difference` defined as the difference between the largest and smallest of :math:`P[h(X)=1 | A=a, Y=1]`, across all values :math:`a` of the sensitive feature(s).",
        "11": "Explained variance regression score function.\n\nBest possible score is 1.0, lower values are worse.",
        "12": "Also known as balanced F-score or F-measure, the F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n\n.. math::\n\n   \\text{False Positive Rate} = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}",
        "13": "False discovery rate is intuitively the rate at which the classifier will be wrong when labeling an example as positive.\n\nThe best value is 0 and the worst value is 1.\n\n.. math::\n\n   \\text{False Discovery Rate} = \\frac{False \\ Positives}{False \\ Positives + True \\ Positives}",
        "14": "False negative rate  is defined as follows:\n\n.. math::\n\n   \\text{False Negative Rate} = \\frac{False \\ Negatives}{False \\ Negatives + True \\ Positives}",
        "15": "The false omission rate is intuitively the rate at which the classifier will be wrong when labeling an example as negative.\n\nThe best value is 0 and the worst value is 1.\n\n.. math::\n\n   \\text{False Omission Rate} = \\frac{False \\ Negatives}{False \\ Negatives + True \\ Negatives}",
        "16": "False positive rate is defined as follows:\n\n.. math::\n\n   \\text{False Positive Rate} = \\frac{False \\ Positives}{False \\ Positives + True \\ Negatives}",
        "17": "The Matthews correlation coefficient is a measure of the quality of a classification model. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes.",
        "18": "Max error the maximum residual error, a metric that captures the worst case error between the predicted value and the true value.\n\nIn a perfectly fitted single output regression model, ``max_error`` would be 0 on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted.",
        "19": "Mean absolute error is the expected value of the absolute error loss or l1-norm loss.",
        "20": "Mean absolute percentage error is an evaluation metric for regression problems.\n\nThe idea of this metric is to be sensitive to relative errors. It is for example not changed by a global scaling of the target variable.",
        "21": "Mean Gamma deviance is the mean `Tweedie deviance <https:\/\/en.wikipedia.org\/wiki\/Tweedie_distribution#The_Tweedie_deviance>`__ error with a power parameter 2. This is a metric that elicits predicted expectation values of regression targets.",
        "22": "Mean pinball loss is used to evaluate the predictive performance of quantile regression models. The pinball loss is equivalent to mean_absolute_error when the quantile parameter alpha is set to 0.5.",
        "23": "Mean Poisson deviance is the mean `Tweedie deviance <https:\/\/en.wikipedia.org\/wiki\/Tweedie_distribution#The_Tweedie_deviance>`__ error with a power parameter 1. This is a metric that elicits predicted expectation values of regression targets.",
        "24": "Mean square error is the expected value of the squared (quadratic) error or loss.",
        "25": "Mean squared log error is the expected value of the squared logarithmic (quadratic) error or loss.",
        "26": "Median absolute error the median of all absolute differences between the target and the prediction. It is robust to outliers.",
        "27": "Membership inference attack occurs when an attacker with black-box access to a model attempts to infer if a data sample was in the model's training dataset or not.\n\nMembership inference attack score is the accuracy score of this binary classification task on a balanced dataset.\n\nThe best value is 0 and the worst value is 1.",
        "28": "This is the mean of the error where any negative errors (i.e., underpredictions) are set to zero.",
        "29": "Precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n\n.. math::\n\n   \\text{Precision} = \\frac{True \\ Positives}{True \\ Positives + False \\ Positives}",
        "30": ":math:`R^2` (coefficient of determination) regression score function.\n\nBest possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a :math:`R^2` score of 0.0.",
        "31": "ROC-AUC score is the area Under the Receiver Operating Characteristic Curve from prediction scores.\n\nROC-AUC varies between 0 and 1 (ideal) \u2014 with an uninformative classifier yielding 0.5.",
        "32": "Root mean square error is the root of ``mean_squared_error`` metric.",
        "33": "Selection rate is the fraction of predicted labels matching the \"good\" outcome.",
        "34": "Sensitive feature prediction score quantifies how much a model redundantly encoded a sensitive feature.\n\nTo evaluate this, a model is trained that tries to predict the sensitive feature from the dataset.\n\nThe score ranges from 0.5 - 1.0. If the score is 0.5, the model is random, and no information about the sensitive feature is likely contained in the dataset. A value of 1 means the sensitive feature is able to be perfectly reconstructed.",
        "35": "The two-sample Kolmogorov-Smirnov test (two-sided) statistic for target and prediction arrays\n    \nThe test compares the underlying continuous distributions F(x) and G(x) of two independent samples.\nThe null hypothesis is that the two distributions are identical, F(x)=G(x)\nIf the KS statistic is small or the p-value is high, then we cannot reject the null hypothesis in favor of the alternative.\n\nFor practical purposes, if the statistic value is higher than `the critical value <https:\/\/sparky.rice.edu\/\/astr360\/kstest.pdf>`__, the two distributions are different.",
        "36": "True negative rate (also called specificity or selectivity) refers to the probability of a negative test, conditioned on truly being negative.\n\n.. math::\n\n   \\text{True Negative Rate} = \\frac{True \\ Negatives}{True \\ Negatives + False \\ Positives }",
        "37": "True Positive Rate (also called sensitivity, recall, or hit rate) refers to the probability of a positive test, conditioned on truly being positive.",
        "38": "This is the mean of the error where any positive errors (i.e. overpredictions) are set to zero.\n\nThe absolute value of the underpredictions is used, so the returned value is always positive."
    },
    "rai_dimension": {
        "0": "performance",
        "1": "privacy",
        "2": "performance",
        "3": "performance",
        "4": "performance",
        "5": "fairness",
        "6": "fairness",
        "7": "security",
        "8": "security",
        "9": "fairness",
        "10": "fairness",
        "11": "performance",
        "12": "performance",
        "13": "performance",
        "14": "performance",
        "15": "performance",
        "16": "performance",
        "17": "performance",
        "18": "performance",
        "19": "performance",
        "20": "performance",
        "21": "performance",
        "22": "performance",
        "23": "performance",
        "24": "performance",
        "25": "performance",
        "26": "performance",
        "27": "privacy",
        "28": "performance",
        "29": "performance",
        "30": "performance",
        "31": "performance",
        "32": "performance",
        "33": "performance",
        "34": "performance",
        "35": "performance",
        "36": "performance",
        "37": "performance",
        "38": "performance"
    },
    "url": {
        "0": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html",
        "1": "https:\/\/github.com\/credo-ai\/credoai_lens\/blob\/develop\/credoai\/evaluators\/privacy.py",
        "2": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.average_precision_score.html",
        "3": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.balanced_accuracy_score.html",
        "4": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.d2_tweedie_score.html",
        "5": "https:\/\/fairlearn.org\/v0.4.6\/api_reference\/fairlearn.metrics.html#fairlearn.metrics.demographic_parity_difference",
        "6": "https:\/\/fairlearn.org\/v0.4.6\/api_reference\/fairlearn.metrics.html#fairlearn.metrics.demographic_parity_ratio",
        "7": "https:\/\/github.com\/credo-ai\/credoai_lens\/blob\/main\/credoai\/evaluators\/security.py",
        "8": "https:\/\/github.com\/credo-ai\/credoai_lens\/blob\/main\/credoai\/evaluators\/security.py",
        "9": "https:\/\/fairlearn.org\/v0.4.6\/api_reference\/fairlearn.metrics.html#fairlearn.metrics.equalized_odds_difference",
        "10": "https:\/\/github.com\/credo-ai\/credoai_lens\/blob\/develop\/credoai\/metrics\/metrics_credoai.py",
        "11": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.explained_variance_score.html",
        "12": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html",
        "13": "https:\/\/github.com\/credo-ai\/credoai_lens\/blob\/develop\/credoai\/metrics\/metrics_credoai.py",
        "14": "https:\/\/fairlearn.org\/v0.4.6\/api_reference\/fairlearn.metrics.html#fairlearn.metrics.false_negative_rate",
        "15": "https:\/\/github.com\/credo-ai\/credoai_lens\/blob\/develop\/credoai\/metrics\/metrics_credoai.py",
        "16": "https:\/\/fairlearn.org\/v0.4.6\/api_reference\/fairlearn.metrics.html#fairlearn.metrics.false_positive_rate",
        "17": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.matthews_corrcoef.html",
        "18": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.max_error.html",
        "19": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_absolute_error.html",
        "20": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_absolute_percentage_error.html",
        "21": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_gamma_deviance.html",
        "22": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_pinball_loss.html",
        "23": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_poisson_deviance.html",
        "24": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_error.html",
        "25": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_log_error.html",
        "26": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.median_absolute_error.html",
        "27": "https:\/\/github.com\/credo-ai\/credoai_lens\/blob\/develop\/credoai\/evaluators\/privacy.py",
        "28": "https:\/\/github.com\/fairlearn\/fairlearn\/blob\/main\/fairlearn\/metrics\/_mean_predictions.py",
        "29": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_score.html",
        "30": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.r2_score.html",
        "31": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html",
        "32": "https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_error.html",
        "33": "https:\/\/fairlearn.org\/v0.5.0\/api_reference\/fairlearn.metrics.html#fairlearn.metrics.selection_rate",
        "34": "https:\/\/github.com\/credo-ai\/credoai_lens\/blob\/develop\/credoai\/modules\/dataset_modules\/dataset_fairness.py",
        "35": "https:\/\/github.com\/credo-ai\/credoai_lens\/blob\/develop\/credoai\/modules\/metrics_credoai.py",
        "36": "https:\/\/fairlearn.org\/v0.5.0\/api_reference\/fairlearn.metrics.html#fairlearn.metrics.true_negative_rate",
        "37": "https:\/\/fairlearn.org\/v0.5.0\/api_reference\/fairlearn.metrics.html#fairlearn.metrics.true_positive_rate",
        "38": "https:\/\/github.com\/fairlearn\/fairlearn\/blob\/main\/fairlearn\/metrics\/_mean_predictions.py"
    }
}